{
  "articles/Source/LinqToDB.Templates/README.html": {
    "href": "articles/Source/LinqToDB.Templates/README.html",
    "title": "T4 Models | Linq To DB (aka linq2db)",
    "keywords": "T4 Models T4 models are used to generate POCO's C# code using your database structure. Installation Firstly you should install one of tools packages into your project: Install-Package linq2db.XXX Where XXX is one of supported databases, for example: Install-Package linq2db.SqlServer This also will install needed linq2db packages: linq2db.t4models linq2db But not data provider packages (install them only if needed to compile your project, T4 models ships it's own data provider assemblies). .Net Core specific Because of .Net Core projects do not support NuGet content files all stuff is not copied into project's folder, so to run T4 templates you'll need: open $(SolutionDir).tools\\linq2db.t4models in Explorer copy CopyMe.XXX.Core.tt.txt to your project's folder or subfolder, then you should use it instead of CopyMe.XXX.tt.txt Running After package installing you will see new LinqToDB.Templates folder in your project, this folder contains all needed T4 stuff to generate your model. Also would be created new folder in tour solution: $(SolutionDir).tools\\linq2db.t4models , it is used to store and link assemblies, needed for generation (linq2db.dll and data provider assemblies). To create a data model template take a look at one of the CopyMe.XXX.tt.txt file in your LinqToDB.Templates project folder. Copy this file to needed project location and rename it, like MyModel.tt There are few main steps in this file: Configuring generation process (read below) Loading metadata - this is a call to LoadMatadata() function - it connects to your database and fetches all needed metadata (table structure, views, and so on) Customizing generation process (read below) Calling GenerateModel() - this will run model generation Configuring generation process Use the following initialization before you call the LoadMetadata() method. NamespaceName = \"DataModels\"; // Namespace of the generated classes. DataContextName = null; // DataContext class name. If null - database name + \"DB\". BaseDataContextClass = null; // Base DataContext class name. If null - LinqToDB.Data.DataConnection. GenerateConstructors = true; // Enforce generating DataContext constructors. DefaultConfiguration = null; // Defines default configuration for default DataContext constructor. BaseEntityClass = null; // Base Entity class name. If null - none. DatabaseName = null; // Table database name - [Table(Database=\"DatabaseName\")]. GenerateDatabaseName = false; // Always generate table database name, even though DatabaseName is null. IncludeDefaultSchema = true; // Default schema name is generated - [Table(Database=\"Northwind\", Schema=\"dbo\", Name=\"Customers\")] OneToManyAssociationType = \"IEnumerable<{0}>\"; // One To Many association type (for members only). Change it to \"List<{0}>\" if needed. GenerateAssociations = true; // Enforce generating associations as type members. GenerateBackReferences = true; // Enforce generating backreference associations (affects both members and extensions). GenerateAssociationExtensions = false; // Enforce generating associations as extension methods. NB: this option does not affect GenerateAssociations. This will require linq2db 1.9.0 and above ReplaceSimilarTables = true; // Replaces stored procedure result class names with similar to existing table class names. GenerateFindExtensions = true; // Generates find extension methods based on PKs information. IsCompactColumns = true; // If true, column compact view. PluralizeClassNames = false; // If true, pluralizes table class names. SingularizeClassNames = true; // If true, singularizes table class names. PluralizeDataContextPropertyNames = true; // If true, pluralizes DataContext property names. SingularizeDataContextPropertyNames = false; // If true, singularizes DataContex pProperty names. GenerateDataTypes = false; // If true, generates the DataType/Length/Precision/Scale properties of the Column attribute (unless overriden by the properties below). GenerateDataTypeProperty = null; // If true, generates the DataType property of the Column attribute. If false, excludes generation on the DataType property even if GenerateDataTypes == true. GenerateLengthProperty = null; // If true, generates the Length property of the Column attribute. If false, excludes generation on the Length property even if GenerateDataTypes == true. GeneratePrecisionProperty = null; // If true, generates the Precision property of the Column attribute. If false, excludes generation on the Precision property even if GenerateDataTypes == true. GenerateScaleProperty = null; // If true, generates the Scale property of the Column attribute. If false, excludes generation on the Scale property even if GenerateDataTypes == true. GenerateDbTypes = false; // If true, generates the DbType property of the Column attribute. GenerateObsoleteAttributeForAliases = false; // If true, generates [Obsolete] attribute for aliases. IsCompactColumnAliases = true; // If true, column alias compact view. NormalizeNames = false; // convert some_name to SomeName for types and members GetSchemaOptions.ExcludedSchemas = new[] { \"TestUser\", \"SYSSTAT\" }; // Defines excluded schemas. GetSchemaOptions.IncludedSchemas = new[] { \"TestUser\", \"SYS\" }; // Defines only included schemas. GetSchemaOptions.ExcludedCatalogs = new[] { \"TestUser\", \"SYSSTAT\" }; // Defines excluded catalogs. GetSchemaOptions.IncludedCatalogs = new[] { \"TestUser\", \"SYS\" }; // Defines only included catalogs. GetSchemaOptions.GetAssociationMemberName = key => \"Association_\" + key.MemberName; // Defines custom naming logic for generated associations. // check GetSchemaOptions class for more options Func<string, bool, string> ToValidName = ToValidNameDefault; // Defines function to convert names to valid (My_Table to MyTable) Func<string, bool, string> ConvertToCompilable = ConvertToCompilableDefault; // Converts name to c# compatible. By default removes uncompatible symbols and converts result with ToValidName Func<ForeignKey, string> GetAssociationExtensionSinglularName = GetAssociationExtensionSinglularNameDefault; // Gets singular method extension method name for association Func<ForeignKey, string> GetAssociationExtensionPluralName = GetAssociationExtensionPluralNameDefault; // Gets plural method extension method name for association Provider specific configurations SQL Server bool GenerateSqlServerFreeText = true; // Defines wheather to generate extensions for Free Text search, or not PostgreSQL bool GenerateCaseSensitiveNames = false; // Defines whether to generate case sensitive or insensitive names Sybase bool GenerateSybaseSystemTables = false; // Defines whether to generate Sybase sysobjects tables or not Customizing generation process Use the following code to modify your model before you call the GenerateModel() method. GetTable(\"Person\").TypeName = \"MyName\"; // Replaces table name. GetTable(\"Person\").BaseClass = \"PersonBase, IId\"; // Set base class & interface for type, null to reset GetColumn(\"Person\", \"PersonID\") .MemberName = \"ID\"; // Replaces column PersonID of Person table with ID. GetColumn(\"Person\", \"PasswordHash\").SkipOnUpdate = true; // Set [Column(SkipOnUpdate=true)], same for other column options GetColumn(\"Person\", \"Gender\") .Type = \"global::Model.Gender\"; // Change column type GetFK(\"Orders\", \"FK_Orders_Customers\").MemberName = \"Customers\"; // Replaces association name. GetFK(\"Orders\", \"FK_Orders_Customers\").AssociationType = AssociationType.OneToMany; // Changes association type. SetTable(string tableName, string TypeName = null, string DataContextPropertyName = null) .Column(string columnName, string MemberName = null, string Type = null, bool? IsNullable = null) .FK (string fkName, string MemberName = null, AssociationType? AssociationType = null) ; Model.Usings.Add(\"MyNamespace\"); // Adds using of namespace. // Replaces all the columns where name is 'TableName' + 'ID' with 'ID'. foreach (var t in Tables.Values) foreach (var c in t.Columns.Values) if (c.IsPrimaryKey && c.MemberName == t.TypeName + \"ID\") c.MemberName = \"ID\"; Useful members and data structures Dictionary<string,Table> Tables = new Dictionary<string,Table> (); Dictionary<string,Procedure> Procedures = new Dictionary<string,Procedure>(); Table GetTable (string name); Procedure GetProcedure (string name); Column GetColumn (string tableName, string columnName); ForeignKey GetFK (string tableName, string fkName); ForeignKey GetForeignKey(string tableName, string fkName); public class Table { public string Schema; public string TableName; public string DataContextPropertyName; public bool IsView; public string Description; public string AliasPropertyName; public string AliasTypeName; public string TypeName; public Dictionary<string,Column> Columns; public Dictionary<string,ForeignKey> ForeignKeys; } public partial class Column : Property { public string ColumnName; // Column name in database public bool IsNullable; public bool IsIdentity; public string ColumnType; // Type of the column in database public DbType DbType; public string Description; public bool IsPrimaryKey; public int PrimaryKeyOrder; public bool SkipOnUpdate; public bool SkipOnInsert; public bool IsDuplicateOrEmpty; public string AliasName; public string MemberName; } public enum AssociationType { Auto, OneToOne, OneToMany, ManyToOne, } public partial class ForeignKey : Property { public string KeyName; public Table OtherTable; public List<Column> ThisColumns; public List<Column> OtherColumns; public bool CanBeNull; public ForeignKey BackReference; public string MemberName; public AssociationType AssociationType; } public partial class Procedure : Method { public string Schema; public string ProcedureName; public bool IsFunction; public bool IsTableFunction; public bool IsDefaultSchema; public Table ResultTable; public Exception ResultException; public List<Table> SimilarTables; public List<Parameter> ProcParameters; } public class Parameter { public string SchemaName; public string SchemaType; public bool IsIn; public bool IsOut; public bool IsResult; public int? Size; public string ParameterName; public string ParameterType; public Type SystemType; public string DataType; }"
  },
  "articles/README.html": {
    "href": "articles/README.html",
    "title": "LINQ to DB | Linq To DB (aka linq2db)",
    "keywords": "LINQ to DB LINQ to DB is the fastest LINQ database access library offering a simple, light, fast, and type-safe layer between your POCO objects and your database. Architecturally it is one step above micro-ORMs like Dapper, Massive, or PetaPoco, in that you work with LINQ expressions, not with magic strings, while maintaining a thin abstraction layer between your code and the database. Your queries are checked by the C# compiler and allow for easy refactoring. However, it's not as heavy as LINQ to SQL or Entity Framework. There is no change-tracking, so you have to manage that yourself, but on the positive side you get more control and faster access to your data. In other words LINQ to DB is type-safe SQL . Visit our blog and see Github.io documentation for more details. Code examples and demos can be found here or in tests . T4 model generation help is here . Releases and Roadmap . How to help the project No, this is not the donate link. We do need something really more valuable - your time . If you really want to help us please read this post . Project Build Status Appveyor Travis master latest Feeds NuGet MyGet V2 https://www.myget.org/F/linq2db/api/v2 V3 https://www.myget.org/F/linq2db/api/v3/index.json Let's get started From NuGet : Install-Package linq2db - .NET & .NET Core Configuring connection strings .NET In your web.config or app.config make sure you have a connection string (check this file for supported providers): <connectionStrings> <add name=\"Northwind\" connectionString = \"Server=.\\;Database=Northwind;Trusted_Connection=True;Enlist=False;\" providerName = \"SqlServer\" /> </connectionStrings> .NET Core .Net Core does not support System.Configuration so to configure connection strings you should implement ILinqToDBSettings , for example: public class ConnectionStringSettings : IConnectionStringSettings { public string ConnectionString { get; set; } public string Name { get; set; } public string ProviderName { get; set; } public bool IsGlobal => false; } public class MySettings : ILinqToDBSettings { public IEnumerable<IDataProviderSettings> DataProviders => Enumerable.Empty<IDataProviderSettings>(); public string DefaultConfiguration => \"SqlServer\"; public string DefaultDataProvider => \"SqlServer\"; public IEnumerable<IConnectionStringSettings> ConnectionStrings { get { yield return new ConnectionStringSettings { Name = \"SqlServer\", ProviderName = \"SqlServer\", ConnectionString = @\"Server=.\\;Database=Northwind;Trusted_Connection=True;Enlist=False;\" }; } } } And later just set on program startup before the first query is done (Startup.cs for example): DataConnection.DefaultSettings = new MySettings(); You can also use same for regular .NET. Now let's create a POCO class Important: you also can generate those classes from your database using T4 templates . Demonstration video could be found here . using System; using LinqToDB.Mapping; [Table(Name = \"Products\")] public class Product { [PrimaryKey, Identity] public int ProductID { get; set; } [Column(Name = \"ProductName\"), NotNull] public string Name { get; set; } // ... other columns ... } At this point LINQ to DB doesn't know how to connect to our database or which POCOs go with what database. All this mapping is done through a DataConnection class: public class DbNorthwind : LinqToDB.Data.DataConnection { public DbNorthwind() : base(\"Northwind\") { } public ITable<Product> Product => GetTable<Product>(); public ITable<Category> Category => GetTable<Category>(); // ... other tables ... } We call the base constructor with the \"Northwind\" parameter. This parameter (called configuration name ) has to match the name=\"Northwind\" we defined above in our connection string. We also have to register our Product class we defined above to allow us to write LINQ queries. And now let's get some data: using LinqToDB; using LinqToDB.Common; public static List<Product> All() { using (var db = new DbNorthwind()) { var query = from p in db.Product where p.ProductID > 25 orderby p.Name descending select p; return query.ToList(); } } Make sure you always wrap your DataConnection class (in our case DbNorthwind ) in a using statement. This is required for proper resource management, like releasing the database connections back into the pool. More details Selecting Columns Most times we get the entire row from the database: from p in db.Product where p.ProductID == 5 select p; However, sometimes getting all the fields is too wasteful so we want only certain fields, but still use our POCOs; something that is challenging for libraries that rely on object tracking, like LINQ to SQL. from p in db.Product orderby p.Name descending select new Product { Name = p.Name }; Composing queries Rather than concatenating strings we can 'compose' LINQ expressions. In the example below the final SQL will be different if onlyActive is true or false, or if searchFor is not null. public static List<Product> All(bool onlyActive, string searchFor) { using (var db = new DbNorthwind()) { var products = from p in db.Product select p; if (onlyActive) { products = from p in products where !p.Discontinued select p; } if (searchFor != null) { products = from p in products where p.Name.Contains(searchFor) select p; } return products.ToList(); } } Paging A lot of times we need to write code that returns only a subset of the entire dataset. We expand on the previous example to show what a product search function could look like. Keep in mind that the code below will query the database twice. Once to find out the total number of records, something that is required by many paging controls, and once to return the actual data. public static List<Product> Search(string searchFor, int currentPage, int pageSize, out int totalRecords) { using (var db = new DbNorthwind()) { var products = from p in db.Product select p; if (searchFor != null) { products = from p in products where p.Name.Contains(searchFor) select p; } totalRecords = products.Count(); return products.Skip((currentPage - 1) * pageSize).Take(pageSize).ToList(); } } Joins This assumes we added a Category class, just like we did with the Product class, defined all the fields, and registered it in our DbNorthwind data access class. We can now write an INNER JOIN query like this: from p in db.Product join c in db.Category on p.CategoryID equals c.CategoryID select new Product { Name = p.Name, Category = c }; and a LEFT JOIN query like this: from p in db.Product from c in db.Category.Where(q => q.CategoryID == p.CategoryID).DefaultIfEmpty() select new Product { Name = p.Name, Category = c }; More samples are here Creating your POCOs In the previous example we assign an entire Category object to our product, but what if we want all the fields in our Product class, but we don't want to specify every field by hand? Unfortunately, we cannot write this: from p in db.Product from c in db.Category.Where(q => q.CategoryID == p.CategoryID).DefaultIfEmpty() select new Product(c); The query above assumes the Product class has a constructor that takes in a Category object. The query above won't work, but we can work around that with the following query: from p in db.Product from c in db.Category.Where(q => q.CategoryID == p.CategoryID).DefaultIfEmpty() select Product.Build(p, c); For this to work, we need a function in the Product class that looks like this: public static Product Build(Product product, Category category) { if (product != null) { product.Category = category; } return product; } One caveat with this approach is that if you're using it with composed queries (see example above) the select Build part has to come only in the final select. Insert At some point we will need to add a new Product to the database. One way would be to call the Insert extension method found in the LinqToDB namespace; so make sure you import that. using (var db = new DbNorthwind()) { db.Insert(product); } This inserts all the columns from our Product class, but without retrieving the generated identity value. To do that we can use InsertWith*Identity methods, like this: using (var db = new DbNorthwind()) { product.ProductID = db.InsertWithInt32Identity(product); } There is also InsertOrReplace that updates a database record if it was found by primary key or adds it otherwise. If you need to insert only certain fields, or use values generated by the database, you could write: using (var db = new DbNorthwind()) { db.Product .Value(p => p.Name, product.Name) .Value(p => p.UnitPrice, 10.2m) .Value(p => p.Added, () => Sql.CurrentTimestamp) .Insert(); } Use of this method also allows us to build insert statements like this: using (var db = new DbNorthwind()) { var statement = db.Product .Value(p => p.Name, product.Name) .Value(p => p.UnitPrice, 10.2m); if (storeAdded) statement.Value(p => p.Added, () => Sql.CurrentTimestamp); statement.Insert(); } Update Updating records follows similar pattern to Insert. We have an extension method that updates all the columns in the database: using (var db = new DbNorthwind()) { db.Update(product); } And we also have a lower level update mechanism: using (var db = new DbNorthwind()) { db.Product .Where(p => p.ProductID == product.ProductID) .Set(p => p.Name, product.Name) .Set(p => p.UnitPrice, product.UnitPrice) .Update(); } Similarly, we can break an update query into multiple pieces if needed: using (var db = new DbNorthwind()) { var statement = db.Product .Where(p => p.ProductID == product.ProductID) .Set(p => p.Name, product.Name); if (updatePrice) statement = statement.Set(p => p.UnitPrice, product.UnitPrice); statement.Update(); } You're not limited to updating a single record. For example, we could discontinue all the products that are no longer in stock: using (var db = new DbNorthwind()) { db.Product .Where(p => p.UnitsInStock == 0) .Set(p => p.Discontinued, true) .Update(); } Delete Similar to how you update records, you can also delete records: using (var db = new DbNorthwind()) { db.Product .Where(p => p.Discontinued) .Delete(); } Bulk Copy Bulk copy feature supports the transfer of large amounts of data into a table from another data source. For faster data inserting DO NOT use a transaction. If you use a transaction an adhoc implementation of the bulk copy feature has been added in order to insert multiple lines at once. You get faster results then inserting lines one by one, but it's still slower than the database provider bulk copy. So, DO NOT use transactions whenever you can (Take care of unique constraints, primary keys, etc. since bulk copy ignores them at insertion). [Table(Name = \"ProductsTemp\")] public class ProductTemp { public int ProductID { get; set; } [Column(Name = \"ProductName\"), NotNull] public string Name { get; set; } // ... other columns ... } list = List<ProductTemp> using (var db = new DbNorthwind()) { db.BulkCopy(list); } Transactions Using database transactions is easy. All you have to do is call BeginTransaction() on your DataConnection, run one or more queries, and then commit the changes by calling CommitTransaction(). If something happened and you need to roll back your changes you can either call RollbackTransaction() or throw an exception. using (var db = new DbNorthwind()) { db.BeginTransaction(); // ... select / insert / update / delete ... if (somethingIsNotRight) { db.RollbackTransaction(); } else { db.CommitTransaction(); } } Also, you can use .NET built-in TransactionScope class: // don't forget that isolation level is serializable by default using (var transaction = new TransactionScope()) { using (var db = new DbNorthwind()) { ... } transaction.Complete(); } Merge Here you can read about MERGE support. Window (Analytic) Functions Here you can read about Window (Analytic) Functions support. MiniProfiler If you would like to use MiniProfiler from StackExchange you'd need to wrap ProfiledDbConnection around our regular DataConnection. public class DbDataContext : DataConnection { #if !DEBUG public DbDataContext() : base(\"Northwind\") { } #else public DbDataContext() : base(GetDataProvider(), GetConnection()) { } private static IDataProvider GetDataProvider() { // you can move this line to other place, but it should be // allways set before LINQ to DB provider instance creation LinqToDB.Common.Configuration.AvoidSpecificDataProviderAPI = true; return new SqlServerDataProvider(\"\", SqlServerVersion.v2012); } private static IDbConnection GetConnection() { var dbConnection = new SqlConnection(@\"Server=.\\SQL;Database=Northwind;Trusted_Connection=True;Enlist=False;\"); return new StackExchange.Profiling.Data.ProfiledDbConnection(dbConnection, MiniProfiler.Current); } #endif } This assumes that you only want to use MiniProfiler while in DEBUG mode and that you are using SQL Server for your database. If you're using a different database you would need to change GetDataProvider() to return the appropriate IDataProvider. For example for MySql you would use: private static IDataProvider GetDataProvider() { return new LinqToDB.DataProvider.MySql.MySqlDataProvider(); }"
  },
  "articles/sql/Window-Functions-(Analytic-Functions).html": {
    "href": "articles/sql/Window-Functions-(Analytic-Functions).html",
    "title": "| Linq To DB (aka linq2db)",
    "keywords": "Support of Window Functions also known as Analytic Functions in LINQ To DB is based on Oracle's Documentation and all mentioned functions are supported. Window functions are implemented as extension methods for static Sql.Ext property. For defining Partitioning and Ordering fluent syntax is used and it is closest as possible to original SQL syntax. C# Syntax: Sql.Ext.[Function]([Parameters]) .Over() .[PartitionPart] .[OrderByPart] .[WindowingPart] .ToValue(); Last function in method chain must be function ToValue() - it is a mark that method chain is finished and provides correct DataType for resulting columns. Example: var q = from p in db.Parent join c in db.Child on p.ParentID equals c.ParentID select new { Rank = Sql.Ext.Rank() .Over() .PartitionBy(p.Value1, c.ChildID) .OrderBy(p.Value1) .ThenBy(c.ChildID) .ThenBy(c.ParentID) .ToValue(), RowNumber = Sql.Ext.RowNumber() .Over() .PartitionBy(p.Value1, c.ChildID) .OrderByDesc(p.Value1) .ThenBy(c.ChildID) .ThenByDesc(c.ParentID) .ToValue(), DenseRank = Sql.Ext.DenseRank() .Over() .PartitionBy(p.Value1, c.ChildID) .OrderBy(p.Value1) .ToValue(), Sum = Sql.Ext.Sum(p.Value1) .Over() .PartitionBy(p.Value1, c.ChildID) .OrderBy(p.Value1) .ToValue(), Avg = Sql.Ext.Average<double>(p.Value1) .Over() .PartitionBy(p.Value1, c.ChildID) .OrderBy(p.Value1) .ToValue(), Count = Sql.Ext.Count(p.ParentID, Sql.AggregateModifier.All) .Over() .PartitionBy(p.Value1) .OrderBy(p.Value1) .Range.Between.UnboundedPreceding.And.CurrentRow .ToValue(), }; var res = q.ToArray(); Resulting SQL: SELECT RANK() OVER(PARTITION BY [p].[Value1], [c7].[ChildID] ORDER BY [p].[Value1], [c7].[ChildID], [c7].[ParentID]) as [c1], ROW_NUMBER() OVER(PARTITION BY [p].[Value1], [c7].[ChildID] ORDER BY [p].[Value1] DESC, [c7].[ChildID], [c7].[ParentID] DESC) as [c2], DENSE_RANK() OVER(PARTITION BY [p].[Value1], [c7].[ChildID] ORDER BY [p].[Value1]) as [c3], SUM([p].[Value1]) OVER(PARTITION BY [p].[Value1], [c7].[ChildID] ORDER BY [p].[Value1]) as [c4], AVG([p].[Value1]) OVER(PARTITION BY [p].[Value1], [c7].[ChildID] ORDER BY [p].[Value1]) as [c5], COUNT(ALL [p].[ParentID]) OVER(PARTITION BY [p].[Value1] ORDER BY [p].[Value1] RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as [c6] FROM [Parent] [p] INNER JOIN [Child] [c7] ON [p].[ParentID] = [c7].[ParentID] Note There is no limitation in window functions usage. LINQ To DB will create SQL and run query, if function is not supported or some part of function is limited in particular Database - error will be thrown on database side. Functions mapping The following table contains list of supported Window Functions and LINQ To DB representation of these functions. Some functions have overloads for supporting full Window Functions syntax. SQL Function Name Linq2db Function Name AVG Sql.Ext.Average() CORR Sql.Ext.Corr() COUNT Sql.Ext.Count() COVAR_POP Sql.Ext.CovarPop() COVAR_SAMP Sql.Ext.CovarSamp() CUME_DIST Sql.Ext.CumeDist() DENSE_RANK Sql.Ext.DenseRank() FIRST Sql.Ext.[AggregateFunction].KeepFirst() FIRST_VALUE Sql.Ext.FirstValue() LAG Sql.Ext.Lag() LAST Sql.Ext.[AggregateFunction].KeepLast() LAST_VALUE Sql.Ext.LastValue() LEAD Sql.Ext.Lead() LISTAGG Sql.Ext.ListAgg() MAX Sql.Ext.Max() MEDIAN Sql.Ext.Median() MIN Sql.Ext.Min() NTH_VALUE Sql.Ext.NthValue() NTILE Sql.Ext.NTile() PERCENT_RANK Sql.Ext.PercentRank() PERCENTILE_CONT Sql.Ext.PercentileCont() PERCENTILE_DISC Sql.Ext.PercentileDisc() RANK Sql.Ext.Rank() RATIO_TO_REPORT Sql.Ext.RatioToReport() REGR_ (Linear Regression) Functions REGR_SLOPE Sql.Ext.RegrSlope() REGR_INTERCEPT Sql.Ext.RegrIntercept() REGR_COUNT Sql.Ext.RegrCount() REGR_R2 Sql.Ext.RegrR2() REGR_AVGX Sql.Ext.RegrAvgX() REGR_AVGY Sql.Ext.RegrAvgY() REGR_SXX Sql.Ext.RegrSXX() REGR_SYY Sql.Ext.RegrSYY() REGR_SXY Sql.Ext.RegrSXY() ROW_NUMBER Sql.Ext.RowNumber() STDDEV Sql.Ext.StdDev() STDDEV_POP Sql.Ext.StdDevPop() STDDEV_SAMP Sql.Ext.StdDevSamp() SUM Sql.Ext.Sum() VAR_POP Sql.Ext.VarPop() VAR_SAMP Sql.Ext.VarSamp() VARIANCE Sql.Ext.Variance() If you have found that your database supports function that is not listed in table above, you can easily create your own extension. Code samples are located in Sql.Analytic.cs Engines that support Window Functions Oracle MSSQL Postresql MariaDB MySQL 8 DB2 z/OS DB2 LUW DB2 iSeries Informix SAP HANA SAP ASE Firebird 3"
  },
  "articles/sql/Join-Operators.html": {
    "href": "articles/sql/Join-Operators.html",
    "title": "Joins | Linq To DB (aka linq2db)",
    "keywords": "Joins LINQ To DB supports full set of joins: INNER, LEFT, FULL, RIGHT, CROSS JOIN. INNER JOIN Join operator on single column var query = from c in db.Category join p in db.Product on c.CategoryID equals p.CategoryID where !p.Discontinued select c; Using \"Where\" condition var query = from c in db.Category from p in db.Product.Where(pr => pr.CategoryID == c.CategoryID) where !p.Discontinued select c; Using \"InnerJoin\" function var query = from c in db.Category from p in db.Product.InnerJoin(pr => pr.CategoryID == c.CategoryID) where !p.Discontinued select c; Result SQL SELECT [c].[CategoryID], [c].[CategoryName], [c].[Description], [c].[Picture] FROM [Categories] [c] INNER JOIN [Products] [p] ON [c].[CategoryID] = [p].[CategoryID] WHERE [p].[Discontinued] <> 1 Join operator on multiple columns var query = from p in db.Product from o in db.Order join d in db.OrderDetail on new { p.ProductID, o.OrderID } equals new { d.ProductID, d.OrderID } where !p.Discontinued select new { p.ProductID, o.OrderID, }; Result SQL SELECT [t3].[ProductID] as [ProductID1], [t3].[OrderID] as [OrderID1] FROM ( SELECT [t1].[ProductID], [t2].[OrderID], [t1].[Discontinued] FROM [Products] [t1], [Orders] [t2] ) [t3] INNER JOIN [Order Details] [d] ON [t3].[ProductID] = [d].[ProductID] AND [t3].[OrderID] = [d].[OrderID] WHERE [t3].[Discontinued] <> 1 LEFT JOIN Join operator on single column var query = from c in db.Category join p in db.Product on c.CategoryID equals p.CategoryID into lj from lp in lj.DefaultIfEmpty() where !lp.Discontinued select c; Using \"Where\" condition var query = from c in db.Category from lp in db.Product.Where(p => p.CategoryID == c.CategoryID).DefaultIfEmpty() where !lp.Discontinued select c; Using \"LeftJoin\" function var query = from c in db.Category from p in db.Product.LeftJoin(pr => pr.CategoryID == c.CategoryID) where !p.Discontinued select c; Result SQL SELECT [c1].[CategoryID], [c1].[CategoryName], [c1].[Description], [c1].[Picture] FROM [Categories] [c1] LEFT JOIN [Products] [lj] ON [c1].[CategoryID] = [lj].[CategoryID] WHERE 1 <> [lj].[Discontinued] RIGHT JOIN Using \"RightJoin\" function var query = from c in db.Category from p in db.Product.RightJoin(pr => pr.CategoryID == c.CategoryID) where !p.Discontinued select c; Result SQL SELECT [t2].[CategoryID], [t2].[CategoryName], [t2].[Description], [t2].[Picture] FROM [Categories] [t2] RIGHT JOIN [Products] [t1] ON [t1].[CategoryID] = [t2].[CategoryID] WHERE 1 <> [t1].[Discontinued] FULL JOIN Using \"FullJoin\" function var query = from c in db.Category from p in db.Product.FullJoin(pr => pr.CategoryID == c.CategoryID) where !p.Discontinued select c; Result SQL SELECT [t2].[CategoryID], [t2].[CategoryName], [t2].[Description], [t2].[Picture] FROM [Categories] [t2] FULL JOIN [Products] [t1] ON [t1].[CategoryID] = [t2].[CategoryID] WHERE 1 <> [t1].[Discontinued] CROSS JOIN Using SelectMany var query = from c in db.Category from p in db.Product where !p.Discontinued select new {c, p}; Result SQL SELECT [t1].[CategoryID], [t1].[CategoryName], [t1].[Description], [t1].[Picture], [t2].[ProductID], [t2].[ProductName], [t2].[SupplierID], [t2].[CategoryID] as [CategoryID1], [t2].[QuantityPerUnit], [t2].[UnitPrice], [t2].[UnitsInStock], [t2].[UnitsOnOrder], [t2].[ReorderLevel], [t2].[Discontinued] FROM [Categories] [t1], [Products] [t2] WHERE 1 <> [t2].[Discontinued]"
  },
  "articles/sql/CTE.html": {
    "href": "articles/sql/CTE.html",
    "title": "Common Table Expression (CTE) | Linq To DB (aka linq2db)",
    "keywords": "Common Table Expression (CTE) Common Table Expression (CTE) support introduced for supporting advanced SQL techniques in LINQ To DB . See documentation for Transact SQL: WITH common_table_expression When it is useful Reusing same SQL part in complex query Recursive table processing Defining simple CTE CTE in LINQ To DB is also IQueryable . Any IQueryable can be converted to CTE by extension method AsCte(\"optional_name\") . var employeeSubordinatesReport = from e in db.Employee select new { e.EmployeeID, e.LastName, e.FirstName, NumberOfSubordinates = db.Employee .Where(e2 => e2.ReportsTo == e.ReportsTo) .Count(), e.ReportsTo }; var employeeSubordinatesReportCte = employeeSubordinatesReport .AsCte(\"EmployeeSubordinatesReport\"); Variable employeeSubordinatesReportCte can be reused in other parts of linq query. var result = from employee in employeeSubordinatesReportCte from manager in employeeSubordinatesReportCte .LeftJoin(manager => employee.ReportsTo == manager.EmployeeID) select new { employee.LastName, employee.FirstName, employee.NumberOfSubordinates, ManagerLastName = manager.LastName, ManagerFirstName = manager.FirstName, ManagerNumberOfSubordinates = manager.NumberOfSubordinates }; You are not limited in defining as many CTEs as you need and they can reference each other. LINQ To DB will put them in correct order and generate SQL with one limitation - there should be no circular references between CTEs . WITH [EmployeeSubordinatesReport] ( [ReportsTo], [EmployeeID], [LastName], [FirstName], [NumberOfSubordinates] ) AS ( SELECT [t2].[ReportsTo], [t2].[EmployeeID], [t2].[LastName], [t2].[FirstName], ( SELECT Count(*) FROM [Employees] [t1] WHERE [t1].[ReportsTo] IS NULL AND [t2].[ReportsTo] IS NULL OR [t1].[ReportsTo] = [t2].[ReportsTo] ) as [c1] FROM [Employees] [t2] ) SELECT [t3].[LastName] as [LastName1], [t3].[FirstName] as [FirstName1], [t3].[NumberOfSubordinates], [manager].[LastName] as [LastName2], [manager].[FirstName] as [FirstName2], [manager].[NumberOfSubordinates] as [NumberOfSubordinates1] FROM [EmployeeSubordinatesReport] [t3] LEFT JOIN [EmployeeSubordinatesReport] [manager] ON [t3].[ReportsTo] = [manager].[EmployeeID] Defining recursive CTE Recursive CTEs are special in the sense they are allowed to reference themselves! Because of this special ability, you can use recursive CTEs to solve problems other queries cannot. Recursive CTEs are really good at working with hierarchical data such as org charts for bill of materials. Recursive CTEs Explained CTEs have limitations that are not handled by LINQ To DB , so you have to be aware of them before start of usage - Guidelines for Defining and Using Recursive Common Table Expressions Since in C# language we can not use variable's reference in it's own initialization part, we have created function that helps in defining such queries GetCte<TCteProjection>(cte => ...) . TCteProjection is required generic parameter that is needed for resolving type of lambda parameter. The following example shows how to define CTE for calculation of employee hierarchy level // defining class for representing Recursive CTE class EmployeeHierarchyCTE { public int EmployeeID; public string LastName; public string FirstName; public int? ReportsTo; public int HierarchyLevel; } using (var db = new NorthwindDB(context)) { var employeeHierarchyCte = db.GetCte<EmployeeHierarchyCTE>(employeeHierarchy => { return ( from e in db.Employee where e.ReportsTo == null select new EmployeeHierarchyCTE { EmployeeID = e.EmployeeID, LastName = e.LastName, FirstName = e.FirstName, ReportsTo = e.ReportsTo, HierarchyLevel = 1 } ) .Concat ( from e in db.Employee from eh in employeeHierarchy .InnerJoin(eh => e.ReportsTo == eh.EmployeeID) select new EmployeeHierarchyCTE { EmployeeID = e.EmployeeID, LastName = e.LastName, FirstName = e.FirstName, ReportsTo = e.ReportsTo, HierarchyLevel = eh.HierarchyLevel + 1 } ); }); var result = from eh in employeeHierarchyCte orderby eh.HierarchyLevel, eh.LastName, eh.FirstName select eh; var data = result.ToArray(); } Resulting SQL: WITH [employeeHierarchy] ( [EmployeeID], [LastName], [FirstName], [ReportsTo], [HierarchyLevel] ) AS ( SELECT [t1].[EmployeeID], [t1].[LastName], [t1].[FirstName], [t1].[ReportsTo], 1 as [c1] FROM [Employees] [t1] WHERE [t1].[ReportsTo] IS NULL UNION ALL SELECT [t2].[EmployeeID], [t2].[LastName], [t2].[FirstName], [t2].[ReportsTo], [eh].[HierarchyLevel] + 1 as [c1] FROM [Employees] [t2] INNER JOIN [employeeHierarchy] [eh] ON [t2].[ReportsTo] = [eh].[EmployeeID] ) SELECT [t3].[EmployeeID] as [EmployeeID2], [t3].[LastName] as [LastName2], [t3].[FirstName] as [FirstName2], [t3].[ReportsTo] as [ReportsTo2], [t3].[HierarchyLevel] FROM [employeeHierarchy] [t3] ORDER BY [t3].[HierarchyLevel], [t3].[LastName], [t3].[FirstName] Database engines that support CTE Database Engine Minimal version Firebird 2.1 MS SQL 2008 MySQL 8.0.1 Oracle 11g Release 2 PostgreSQL 8.4 SQLite 3.8.3 IBM DB2 8 Known limitations Oracle and Firebird DML operations that use CTE is not completely implemented. TBD"
  },
  "articles/sql/Bulk-Copy.html": {
    "href": "articles/sql/Bulk-Copy.html",
    "title": "Bulk Copy (Bulk Insert) | Linq To DB (aka linq2db)",
    "keywords": "Bulk Copy (Bulk Insert) Some database servers provide functionality to quickly insert large amount of data into table. Downside of this method is that each server has it's own view on how this functionality should work and there is no standard interface to it. Overview To leverage complexity of work with this operation, LINQ To DB provides BulkCopy method. There are several overrides of it, but all they do the same - take data and operation options, perform insert and return operation status. How insert operation performed internally depends on provider support level and provided options. // DataConnectionExtensions.cs BulkCopyRowsCopied BulkCopy<T>(this DataConnection dataConnection, BulkCopyOptions options, IEnumerable<T> source) BulkCopyRowsCopied BulkCopy<T>(this DataConnection dataConnection, int maxBatchSize, IEnumerable<T> source) BulkCopyRowsCopied BulkCopy<T>(this DataConnection dataConnection, IEnumerable<T> source) BulkCopyRowsCopied BulkCopy<T>(this ITable<T> table, BulkCopyOptions options, IEnumerable<T> source) BulkCopyRowsCopied BulkCopy<T>(this ITable<T> table, int maxBatchSize, IEnumerable<T> source) BulkCopyRowsCopied BulkCopy<T>(this ITable<T> table, IEnumerable<T> source) Insert methods and support by providers LINQ To DB allows you to specify one of four insert methods (or three, as Default is not an actual method): Default . LINQ To DB will choose method automatically, based on used provider. Which method to use for specific provider could be overriden using <PROVIDER_NAME>Tools.DefaultBulkCopyType property. RowByRow . This method just iterate over provided collection and insert each record using separate SQL INSERT command. Least effective method, but some providers support only this one. MultipleRows . Similar to RowByRow . Inserts multiple records at once using SQL INSERT FROM SELECT or similar batch insert command. This one is faster than RowByRow , but available only for providers that support such INSERT operation. If method is not supported, LINQ To DB silently fallback to RowByRow implementation. ProviderSpecific . Most effective method, available only for few providers. Uses provider specific functionality, usually not based on SQL and could have provider-specific limitations, like transactions support. If method is not supported, LINQ To DB silently fallback to MultipleRows implementation. Provider RowByRow MultipleRows ProviderSpecific Default Notes Microsoft Access Yes No No MultipleRows AccessTools.DefaultBulkCopyType IBM DB2 (LUW, zOS) Yes Yes Yes (will fallback to MultipleRows if called in transaction) MultipleRows DB2Tools.DefaultBulkCopyType Firebird Yes Yes No MultipleRows FirebirdTools.DefaultBulkCopyType IBM Informix Yes No No MultipleRows InformixTools.DefaultBulkCopyType MySql / MariaDB Yes Yes No MultipleRows MySqlTools.DefaultBulkCopyType Oracle Yes Yes Yes (will fallback to MultipleRows if called in transaction) MultipleRows OracleTools.DefaultBulkCopyType PostgreSQL Yes Yes Yes (read important notes below) MultipleRows PostgreSQLTools.DefaultBulkCopyType SAP HANA Yes No Yes MultipleRows SapHanaTools.DefaultBulkCopyType Microsoft SQL CE Yes Yes No MultipleRows SqlCeTools.DefaultBulkCopyType SQLite Yes Yes No MultipleRows SQLiteTools.DefaultBulkCopyType Microsoft SQL Server Yes Yes Yes ProviderSpecific SqlServerTools.DefaultBulkCopyType Sybase ASE Yes Yes No MultipleRows SybaseTools.DefaultBulkCopyType PostgreSQL provider-specific bulk copy For PostgreSQL BulkCopy use BINARY COPY operation when ProviderSpecific method specified. This operation is very sensitive to what types are used. You must always use proper type that match type in target table, or you will receive error from server (e.g. \"22P03: incorrect binary data format\" ). Below is a list of types, that could result in error without explicit type specification: decimal / numeric vs money . Those are two different types, mapped to System.Decimal . Default mappings will use numeric type, so if your column has money type, you should type it in mapping using DataType = DataType.Money or DbType = \"money\" hints. time vs interval . Those are two different types, mapped to System.TimeSpan . Default mappings will use time type, so if your column has interval type, you should type it in mapping using DbType = \"interval\" hint. Or use NpgsqlTimeSpan type for intervals. any text types/ json vs jsonb . All those types mapped to System.String (except character which is mapped to System.Char ). Default mappings will not work for jsonb column and you should type it in mapping using DataType = DataType.BinaryJson or DbType = \"jsonb\" hint. inet vs cidr . If you use NpgsqlInet type for mapping column, it could be mapped to both inet and 'cidr' types. There is no default mapping for this type, so you should explicitly specify it using DbType = \"inet\" or DbType = \"cidr\" hint. Also for inet you can use IPAddress which will be mapped to inet type. macaddr vs macaddr8 . Both types could be mapped to the same PhysicalAddress / String types, so you should explicitly specify column type using DbType = \"macaddr\" or DbType = \"macaddr8\" hints. Even if you use provider version without macaddr8 support, you should specify hint or it will break after provider update to newer version. date type. You should use NpgsqlDate type in mapping or specify DataType = DataType.Date or DbType = \"date\" hints. time with time zone type needs DbType = \"time with time zone\" hint. If you have issues with other types, feel free to create an issue. Options See BulkCopyOptions properties and support per-provider KeepIdentity option (default : false ) This option allows to insert provided values into identity column. It is supported by limited set of providers and is not compatible with RowByRow mode. Latter means that if provider doesn't support any other insert mode, KeepIdentity option is not supported too. This option is not supported for RowByRow because corresponding functionality is not implemented by LINQ To DB and could be added on request. If you will set this option to true for unsupported mode or provider, you will get LinqToDBException . Provider Support Microsoft Access No IBM DB2 (LUW, zOS) Only for GENERATED BY DEFAULT columns Firebird No (you need to disable triggers manually, if you use generators in triggers) IBM Informix No MySql / MariaDB Yes Oracle Partial. Starting from version 12c it will work for GENERATED BY DEFAULT columns (as DB2), for earlier versions you need to disable triggers with generators (as Firebird). Note that for versions prior to 12c, no exception will be thrown if you will try to use it with KeepIdentity set to true and generated values will be used silently as LINQ To DB don't have Oracle version detection right now. This could be changed in future. PostgreSQL Yes SAP HANA Depends on provider version (HANA 2 only?) Microsoft SQL CE Yes SQLite Yes Microsoft SQL Server Yes Sybase ASE Yes See Also As an alternative to BulkCopy , Merge operation could be used. It allows more flexibility but not available for some providers and will be always slower than BulkCopy with native provider support."
  },
  "articles/releasenotes/Unreleased-Changes.html": {
    "href": "articles/releasenotes/Unreleased-Changes.html",
    "title": "Will be included into next post-2.0 release | Linq To DB (aka linq2db)",
    "keywords": "This page contains changes and fixes that were not inluded in any release yet and available only through MyGet feed. Will be included into next post-2.0 release none yet"
  },
  "articles/project/Issue-reporting.html": {
    "href": "articles/project/Issue-reporting.html",
    "title": "How to report an issue | Linq To DB (aka linq2db)",
    "keywords": "How to report an issue To help you with your problem we need to know: linq2db version you are using Database you are using Code sample, demonstrating the problem & result SQL query (if any) Explain what is wrong Certainly, the best way of reporting an issue would be the Pull Request with test, demonstrating an issue and fix. Or just the test. Please, when making such PR use data model from Tests.Model project. If your query is not obvious and it is not clear how to write minimal reproducing sample, please read above about how to generate test sample. Generating the test This page describes how to generate NUnit test, demonstrating your issue. Cleanup C:\\Users\\[username]\\AppData\\Local\\Temp\\linq2db (if exists) Set LinqToDB.Common.Configuration.Linq.GenerateExpressionTest = true; before your failing query, and LinqToDB.Common.Configuration.Linq.GenerateExpressionTest = false; after. Execute your failing query. ExpressionTest.0.cs file would be generated in C:\\Users\\[username]\\AppData\\Local\\Temp\\linq2db . This would contain unit test with your query and POCO model. Attach this file to the issue. For example: LinqToDB.Common.Configuration.Linq.GenerateExpressionTest = true; // Don't forget to trigger query execution by calling e.g. ToList() var q = db.GetTable<MyTable>().Where(_ => _.Id > 111).ToList(); LinqToDB.Common.Configuration.Linq.GenerateExpressionTest = false;"
  },
  "articles/project/How-can-i-help.html": {
    "href": "articles/project/How-can-i-help.html",
    "title": "How can I help? | Linq To DB (aka linq2db)",
    "keywords": "How can I help? Certainly the best help will be pull requests with fixes (please read this guide before creating PR). If you are not ready to start as developer you can: Write XML documentation. Yes, this thing we do really need - XML documentation with samples. You can take any undocumented public class or method and document it and make pull requests. Stories - you can tell your friends about linq2db, also you can post some samples and guides, it would be really great. Testing! It is really great if you can spent some of your time to run your projects tests with new RC version. Proof-reading. If you are native english speaker, it would be nice if you can spend some time proof-reading documentation. Can run Linq To DB tests on DB2 z/OS database? Ping us here . Testing how to Open new issue \"Invite me to testers team\", we'll send you an invitation. Before each official release new PR would be created with notification to all team members. Next you need to update to the latest RC package from MyGet and run your tests. If something is wrong please report us and if everything is alright just approve PR."
  },
  "articles/project/contrib.html": {
    "href": "articles/project/contrib.html",
    "title": "Contributing guide | Linq To DB (aka linq2db)",
    "keywords": "Contributing guide Development rules and regulations, code style Follow this document Project structure description Solution and folder structure Folder Description .\\ Root folder .\\Build Various files for AppVeyor builds and common project settings .\\Data Contains test databases creation scripts and database files .\\Doc DocFX documentation files .\\NuGet LINQ to DB NuGet packages build files, readme.txt .\\Redist Redistributable binaries for providers unavailable officially at NuGet .\\Source\\LinqToDB LINQ to DB source code .\\Source\\LinqToDB.Templates LINQ to DB t4models source code .\\Tests Unit tests .\\Tests\\Base LINQ to DB testing framework .\\Tests\\FSharp F# models and tests .\\Tests\\IBM.Core Tests for IBM.Data.DB2.Core provider .\\Tests\\Linq Main project for LINQ to DB unit tests .\\Tests\\Model Model classes for tests .\\Tests\\T4.Linq Models for test databases, generated using t4models .\\Tests\\T4.Model T4Models tests .\\Tests\\T4.Wpf T4Models NotifyPropertyChanged template test project .\\Tests\\TestApp SQL Server spatial types test application .\\Tests\\Tests.Benchmark Benchmark tests .\\Tests\\VisualBasic Visual Basic models and tests Solutions: .\\linq2db.sln - VS2017 solution Projects: Project .NET 4.5 .NET 4.5.2 .NET 4.6 .NET 4.6.2 .NET Standard 1.6 .NET Standard 2.0 .NET Core 1.0 .NET Core 2.0 .\\Source\\LinqToDB\\LinqToDB.csproj √ √ √ √ .\\Tests\\Linq\\Tests.Base.csproj √ √ √ .\\Tests\\IBM.Core\\Tests.IBM.Core.csproj √ √ .\\Tests\\Linq\\Tests.csproj √ √ √ .\\Tests\\FSharp\\Tests.FSharp.fsproj √ √ √ .\\Tests\\Model\\Tests.Model.csproj √ √ .\\Tests\\T4.Linq\\Tests.T4.Linq.csproj √ √ .\\Tests\\T4.Model\\Tests.T4.Model.csproj √ .\\Tests\\T4.Wpf\\Tests.T4.Wpf.csproj √ .\\Tests\\TestApp\\TestApp.csproj √ .\\Tests\\Tests.Benchmark\\Tests.Benchmark.csproj √ .\\Tests\\VisualBasic\\Tests.VisualBasic.vbproj √ √ Building You can use the solution to build and run tests. Also you can build whole solution or library using the following batch files: run .\\Build.cmd - builds all the projects in the solution for Debug, Release, and AppVeyor configurations run .\\Source\\LinqToDB\\Compile.cmd - builds LinqToDB projects for Debug and Release configurations Different platforms support Because of compiling for different platforms we do use: Conditional compilation. Different projects and configurations define compilation symbols: NET45 - .NET 4.5 compatibility level NETSTANDARD1_6 - .NET Standard 1.6 compatibility level NETSTANDARD2_0 - .NET Standard 2.0 compatibility level Implementing missing classes and enums. There are some under .\\Source\\LinqToDB\\Compatibility folder. Branches master - current stable branch release - branch with the latest release release1 - branch for critical fixes for version 1.xx.yy version1 - stable branch for version 1.xx.yy Run tests NUnit3 is used as unit testing framework. Most tests are run for all supported databases, and written in same pattern: [TestFixture] public class Test: TestBase // TestBase - base class, provides base methods and object data sources { // DataContextSourceAttribute - implements NUnit ITestBuilder and provides context values to test // TestAttribute - not required for nunit test runner, but needed for Resharper test runner [Test, DataContextSource] public void Test(string context) { // TestBase.GetDataContext - creates new IDataContext, supports creating WCF client and server using(var db = GetDataContext(context)) { // Here is the most interesting // this.Person - list of persons, corresponding Person table in database (derived from TestBase) // db.Person - database table // So test checks that LINQ to Objects query produces the same result as executed database query AreEqual(this.Person.Where(_ => _.Name == \"John\"), db.Person.Where(_ => _.Name == \"John\")); } } } Configure data providers for tests DataContextSourceAttribute generates tests for each configured data provider, configuration is taken from .\\Tests\\Linq\\DataProviders.json and .\\Tests\\Linq\\UserDataProviders.json if it exists. Linq\\UserDataProviders.json is used to specify user-specific settings such as connections strings to test databases and list of tested providers. The [User]DataProviders.json is a regular JSON file: UserDataProviders.json example (with description) { // .net framework 4.5 test configuration \"NET45\" : { // base configuration to inherit settings from // Inheritance rules: // - DefaultConfiguration, TraceLevel, Providers - use value from base configuration only if it is not defined in current configuration // - Connections - merge current and base connection strings \"BasedOn\" : \"LocalConnectionStrings\", // default provider, used as a source of reference data // LINQ to DB uses SQLite for it and you hardly need to change it \"DefaultConfiguration\" : \"SQLite.Classic\", // logging level // Supported values: Off, Error, Warning, Info, Verbose // Default level: Info \"TraceLevel\" : \"Error\", // list of database providers, enabled for current test configuration \"Providers\" : [ \"Access\", \"SqlCe\", \"SQLite.Classic\", \"SQLite.MS\", \"Northwind.SQLite\", \"Northwind.SQLite.MS\", \"SqlServer\", \"SqlServer.2014\", \"SqlServer.2012\", \"SqlServer.2012.1\", \"SqlServer.2008\", \"SqlServer.2008.1\", \"SqlServer.2005\", \"SqlServer.2005.1\", \"SqlAzure.2012\", \"DB2\", \"Firebird\", \"Informix\", \"MySql\", \"MariaDB\", \"Oracle.Native\", \"Oracle.Managed\", \"PostgreSQL\", \"Sybase\", \"Northwind\", \"TestNoopProvider\" ], // list of test skip categories, disabled for current test configuration // to set test skip category, use SkipCategoryAttribute on test method, class or whole assembly \"Skip\" : [ \"Access.12\" ] }, // .net core 1.0 test configuration \"CORE1\" : { \"BasedOn\" : \"LocalConnectionStrings\", \"Providers\" : [ \"SQLite.MS\", \"Northwind.SQLite.MS\", \"SqlServer\", \"SqlServer.2014\", \"SqlServer.2012\", \"SqlServer.2012.1\", \"SqlServer.2008\", \"SqlServer.2008.1\", \"SqlServer.2005\", \"SqlServer.2005.1\", \"SqlAzure.2012\", \"Firebird\", \"MySql\", \"MariaDB\", \"PostgreSQL\", \"Northwind\", \"TestNoopProvider\" ] }, // .net core 2.0 test configuration \"CORE2\" : { \"BasedOn\" : \"LocalConnectionStrings\", \"Providers\" : [ \"SQLite.MS\", \"Northwind.SQLite.MS\", \"SqlServer\", \"SqlServer.2014\", \"SqlServer.2012\", \"SqlServer.2012.1\", \"SqlServer.2008\", \"SqlServer.2008.1\", \"SqlServer.2005\", \"SqlServer.2005.1\", \"SqlAzure.2012\", \"Firebird\", \"MySql\", \"MariaDB\", \"PostgreSQL\", \"Northwind\", \"TestNoopProvider\" ] }, // list of connection strings for all providers \"LocalConnectionStrings\": { \"BasedOn\" : \"CommonConnectionStrings\", \"Connections\" : { // override connection string for SqlAzure.2012 provider // all other providers will use default inherited connection strings from CommonConnectionStrings configuration \"SqlAzure.2012\" : { \"Provider\" : \"System.Data.SqlClient\", \"ConnectionString\" : \"Server=tcp:xxxxxxxxx.database.windows.net,1433;Database=TestData;User ID=TestUser@zzzzzzzzz;Password=TestPassword;Trusted_Connection=False;Encrypt=True;\" } } } } To define your own configurations DO NOT EDIT DataProviders.json - create .\\Tests\\Linq\\UserDataProviders.json and define needed configurations. Right now tests execution depends on _CreateData.* tests executed first. Those tests recreate test databases and populate them with test data, so if you are going to run one test be sure to run _CreateData before it manually. Also - if your test changes database data, be sure to revert those changes (!) to avoid side effects for other tests. Continuous Integration We do run builds and tests with: AppVeyor (Windows) appveyor.yml . Makes build and runs tests for: .Net 4.5.2: NET45.AppVeyor configuration . Full set of tests are done. .Net Core 1.0: CORE1.AppVeyor configuration . Only _Create tests are done (smoke testing). .Net Core 2.0: CORE2.AppVeyor configuration . Only _Create tests are done (smoke testing). DocFx - to build documentation . Deploy is done only for release branch. Travis (Linux) .travis.yml . Makes build and runs tests for: .Net Core 2.0: CORE2.Travis configuration . Full set of tests are done. CI builds are done only for next branches: master /version.*/ (regex) /release.*/ (regex) /dev.*/ (regex) Skip CI build If you want to skip building commit by CI (for example you have changed *.md files only) begin commit comment with [ci skip] . Publishing packages Release candidate packages are published by AppVeyor to MyGet.org for each successful build of master branch. Release packages are published by AppVeyor to NuGet.org for each successful build of release and release1 branch. Building releases Update Release Notes and create empty entry for vNext release Create PR from master to release branch, in comments add @testers to notify all testers that we are ready to release Wait few days for feedback from testers and approval from contributors Merge PR Tag release Update versions in master branch (this will lead to publish all next master builds as new version RC): in .\\appveyor.yml set assemblyVersion parameter in *.nuspec files update linq2db dependency version in issue template update default linq2db version Process In general you should follow simple rules: Development rules and regulations, code style Do not add new features without tests Avoid direct pushes to master and release branches To fix some issue or implement new feature create new branch and make pull request after you are ready to merge. Merge your PR only after contributor's review. If you are going to implement any big feature you may want other contributors to participate (coding, code review, feature discuss and so on), so to do it: Create new PR with [WIP] prefix (Work In Process) After you are ready to merge remove the prefix & assign contributors as reviewers If you do have write access, it is recommended to use central repository (not forks). Why - simple, it would allow other teammates to help you in developing (if needed). Certainly you are free to use fork if it is more convenient to you Please avoid adding new public classes, properties, methods without XML doc Read issues and help users Do not EF :) See also Issue reporting"
  },
  "articles/get-started/index.html": {
    "href": "articles/get-started/index.html",
    "title": "Getting Started - LINQ To DB | Linq To DB (aka linq2db)",
    "keywords": "Getting Started with LINQ To DB Installing LINQ To DB A summary of the steps necessary to add LINQ To DB to your application in different platforms and popular IDEs. Step-by-step Tutorials These introductory tutorials require no previous knowledge of LINQ To DB or a particular IDE. They will take you step-by-step through creating a simple application that queries and saves data from a database. We have provided tutorials to get you started on various operating systems and application types."
  },
  "articles/general/Video.html": {
    "href": "articles/general/Video.html",
    "title": "LINQ Video | Linq To DB (aka linq2db)",
    "keywords": "LINQ Video LINQ to SqlServer LINQ CRUD Operations"
  },
  "articles/FAQ.html": {
    "href": "articles/FAQ.html",
    "title": "Mapping | Linq To DB (aka linq2db)",
    "keywords": "Mapping How can I use calculated fields? You need to mark them to be ignored during insert or update operations, e.g. using ColumnAttribute attribute: public class MyEntity { [Column(SkipOnInsert = true, SkipOnUpdate = true)] public int CalculatedField { get; set; } }"
  },
  "articles/CONTRIBUTING.html": {
    "href": "articles/CONTRIBUTING.html",
    "title": "Contributing guide | Linq To DB (aka linq2db)",
    "keywords": "Contributing guide Development rules and regulations, code style Follow this document Project structure description Solution and folder structure Folder Description .\\ Root folder .\\Build Various files for AppVeyor builds and common project settings .\\Data Contains test databases creation scripts and database files .\\Doc DocFX documentation files .\\NuGet LINQ to DB NuGet packages build files, readme.txt .\\Redist Redistributable binaries for providers unavailable officially at NuGet .\\Source\\LinqToDB LINQ to DB source code .\\Source\\LinqToDB.Templates LINQ to DB t4models source code .\\Tests Unit tests .\\Tests\\Base LINQ to DB testing framework .\\Tests\\FSharp F# models and tests .\\Tests\\IBM.Core Tests for IBM.Data.DB2.Core provider .\\Tests\\Linq Main project for LINQ to DB unit tests .\\Tests\\Model Model classes for tests .\\Tests\\T4.Linq Models for test databases, generated using t4models .\\Tests\\T4.Model T4Models tests .\\Tests\\T4.Wpf T4Models NotifyPropertyChanged template test project .\\Tests\\TestApp SQL Server spatial types test application .\\Tests\\Tests.Benchmark Benchmark tests .\\Tests\\VisualBasic Visual Basic models and tests Solutions: .\\linq2db.sln - VS2017 solution Projects: Project .NET 4.5 .NET 4.5.2 .NET 4.6 .NET 4.6.2 .NET Standard 1.6 .NET Standard 2.0 .NET Core 1.0 .NET Core 2.0 .\\Source\\LinqToDB\\LinqToDB.csproj √ √ √ √ .\\Tests\\Linq\\Tests.Base.csproj √ √ √ .\\Tests\\IBM.Core\\Tests.IBM.Core.csproj √ √ .\\Tests\\Linq\\Tests.csproj √ √ √ .\\Tests\\FSharp\\Tests.FSharp.fsproj √ √ √ .\\Tests\\Model\\Tests.Model.csproj √ √ .\\Tests\\T4.Linq\\Tests.T4.Linq.csproj √ √ .\\Tests\\T4.Model\\Tests.T4.Model.csproj √ .\\Tests\\T4.Wpf\\Tests.T4.Wpf.csproj √ .\\Tests\\TestApp\\TestApp.csproj √ .\\Tests\\Tests.Benchmark\\Tests.Benchmark.csproj √ .\\Tests\\VisualBasic\\Tests.VisualBasic.vbproj √ √ Building You can use the solution to build and run tests. Also you can build whole solution or library using the following batch files: run .\\Build.cmd - builds all the projects in the solution for Debug, Release, and AppVeyor configurations run .\\Source\\LinqToDB\\Compile.cmd - builds LinqToDB projects for Debug and Release configurations Different platforms support Because of compiling for different platforms we do use: Conditional compilation. Different projects and configurations define compilation symbols: NET45 - .NET 4.5 compatibility level NETSTANDARD1_6 - .NET Standard 1.6 compatibility level NETSTANDARD2_0 - .NET Standard 2.0 compatibility level Implementing missing classes and enums. There are some under .\\Source\\LinqToDB\\Compatibility folder. Branches master - current stable branch release - branch with the latest release release1 - branch for critical fixes for version 1.xx.yy version1 - stable branch for version 1.xx.yy Run tests NUnit3 is used as unit testing framework. Most tests are run for all supported databases, and written in same pattern: [TestFixture] public class Test: TestBase // TestBase - base class, provides base methods and object data sources { // DataContextSourceAttribute - implements NUnit ITestBuilder and provides context values to test // TestAttribute - not required for nunit test runner, but needed for Resharper test runner [Test, DataContextSource] public void Test(string context) { // TestBase.GetDataContext - creates new IDataContext, supports creating WCF client and server using(var db = GetDataContext(context)) { // Here is the most interesting // this.Person - list of persons, corresponding Person table in database (derived from TestBase) // db.Person - database table // So test checks that LINQ to Objects query produces the same result as executed database query AreEqual(this.Person.Where(_ => _.Name == \"John\"), db.Person.Where(_ => _.Name == \"John\")); } } } Configure data providers for tests DataContextSourceAttribute generates tests for each configured data provider, configuration is taken from .\\Tests\\Linq\\DataProviders.json and .\\Tests\\Linq\\UserDataProviders.json if it exists. Linq\\UserDataProviders.json is used to specify user-specific settings such as connections strings to test databases and list of tested providers. The [User]DataProviders.json is a regular JSON file: UserDataProviders.json example (with description) { // .net framework 4.5 test configuration \"NET45\" : { // base configuration to inherit settings from // Inheritance rules: // - DefaultConfiguration, TraceLevel, Providers - use value from base configuration only if it is not defined in current configuration // - Connections - merge current and base connection strings \"BasedOn\" : \"LocalConnectionStrings\", // default provider, used as a source of reference data // LINQ to DB uses SQLite for it and you hardly need to change it \"DefaultConfiguration\" : \"SQLite.Classic\", // logging level // Supported values: Off, Error, Warning, Info, Verbose // Default level: Info \"TraceLevel\" : \"Error\", // list of database providers, enabled for current test configuration \"Providers\" : [ \"Access\", \"SqlCe\", \"SQLite.Classic\", \"SQLite.MS\", \"Northwind.SQLite\", \"Northwind.SQLite.MS\", \"SqlServer\", \"SqlServer.2014\", \"SqlServer.2012\", \"SqlServer.2012.1\", \"SqlServer.2008\", \"SqlServer.2008.1\", \"SqlServer.2005\", \"SqlServer.2005.1\", \"SqlAzure.2012\", \"DB2\", \"Firebird\", \"Informix\", \"MySql\", \"MariaDB\", \"Oracle.Native\", \"Oracle.Managed\", \"PostgreSQL\", \"Sybase\", \"Northwind\", \"TestNoopProvider\" ], // list of test skip categories, disabled for current test configuration // to set test skip category, use SkipCategoryAttribute on test method, class or whole assembly \"Skip\" : [ \"Access.12\" ] }, // .net core 1.0 test configuration \"CORE1\" : { \"BasedOn\" : \"LocalConnectionStrings\", \"Providers\" : [ \"SQLite.MS\", \"Northwind.SQLite.MS\", \"SqlServer\", \"SqlServer.2014\", \"SqlServer.2012\", \"SqlServer.2012.1\", \"SqlServer.2008\", \"SqlServer.2008.1\", \"SqlServer.2005\", \"SqlServer.2005.1\", \"SqlAzure.2012\", \"Firebird\", \"MySql\", \"MariaDB\", \"PostgreSQL\", \"Northwind\", \"TestNoopProvider\" ] }, // .net core 2.0 test configuration \"CORE2\" : { \"BasedOn\" : \"LocalConnectionStrings\", \"Providers\" : [ \"SQLite.MS\", \"Northwind.SQLite.MS\", \"SqlServer\", \"SqlServer.2014\", \"SqlServer.2012\", \"SqlServer.2012.1\", \"SqlServer.2008\", \"SqlServer.2008.1\", \"SqlServer.2005\", \"SqlServer.2005.1\", \"SqlAzure.2012\", \"Firebird\", \"MySql\", \"MariaDB\", \"PostgreSQL\", \"Northwind\", \"TestNoopProvider\" ] }, // list of connection strings for all providers \"LocalConnectionStrings\": { \"BasedOn\" : \"CommonConnectionStrings\", \"Connections\" : { // override connection string for SqlAzure.2012 provider // all other providers will use default inherited connection strings from CommonConnectionStrings configuration \"SqlAzure.2012\" : { \"Provider\" : \"System.Data.SqlClient\", \"ConnectionString\" : \"Server=tcp:xxxxxxxxx.database.windows.net,1433;Database=TestData;User ID=TestUser@zzzzzzzzz;Password=TestPassword;Trusted_Connection=False;Encrypt=True;\" } } } } To define your own configurations DO NOT EDIT DataProviders.json - create .\\Tests\\Linq\\UserDataProviders.json and define needed configurations. Right now tests execution depends on _CreateData.* tests executed first. Those tests recreate test databases and populate them with test data, so if you are going to run one test be sure to run _CreateData before it manually. Also - if your test changes database data, be sure to revert those changes (!) to avoid side effects for other tests. Continuous Integration We do run builds and tests with: AppVeyor (Windows) appveyor.yml . Makes build and runs tests for: .Net 4.5.2: NET45.AppVeyor configuration . Full set of tests are done. .Net Core 1.0: CORE1.AppVeyor configuration . Only _Create tests are done (smoke testing). .Net Core 2.0: CORE2.AppVeyor configuration . Only _Create tests are done (smoke testing). DocFx - to build documentation . Deploy is done only for release branch. Travis (Linux) .travis.yml . Makes build and runs tests for: .Net Core 2.0: CORE2.Travis configuration . Full set of tests are done. CI builds are done only for next branches: master /version.*/ (regex) /release.*/ (regex) /dev.*/ (regex) Skip CI build If you want to skip building commit by CI (for example you have changed *.md files only) begin commit comment with [ci skip] . Publishing packages Release candidate packages are published by AppVeyor to MyGet.org for each successful build of master branch. Release packages are published by AppVeyor to NuGet.org for each successful build of release and release1 branch. Building releases Update Release Notes and create empty entry for vNext release Create PR from master to release branch, in comments add @testers to notify all testers that we are ready to release Wait few days for feedback from testers and approval from contributors Merge PR Tag release Update versions in master branch (this will lead to publish all next master builds as new version RC): in .\\appveyor.yml set assemblyVersion parameter in *.nuspec files update linq2db dependency version in issue template update default linq2db version Process In general you should follow simple rules: Development rules and regulations, code style Do not add new features without tests Avoid direct pushes to master and release branches To fix some issue or implement new feature create new branch and make pull request after you are ready to merge. Merge your PR only after contributor's review. If you are going to implement any big feature you may want other contributors to participate (coding, code review, feature discuss and so on), so to do it: Create new PR with [WIP] prefix (Work In Process) After you are ready to merge remove the prefix & assign contributors as reviewers If you do have write access, it is recommended to use central repository (not forks). Why - simple, it would allow other teammates to help you in developing (if needed). Certainly you are free to use fork if it is more convenient to you Please avoid adding new public classes, properties, methods without XML doc Read issues and help users Do not EF :)"
  },
  "articles/sql/merge/Merge-API.html": {
    "href": "articles/sql/merge/Merge-API.html",
    "title": "Merge API | Linq To DB (aka linq2db)",
    "keywords": "Merge API This API available since linq2db 1.9.0. It superseeds previous version of API with very limited functionality. For migration from old API check link below. Supported Databases Microsoft SQL Server IBM DB2 Firebird Oracle Database Sybase/SAP ASE IBM Informix SAP HANA 2 Related Pages Background API Description Migration from old API guide Introduction Merge is an atomic operation to update table (target) content using other table (source). Merge API provides methods to build Merge command and execute it. Command could have following elements (availability depends on database engine, see [[support table|Merge-API-:-Background-Information-and-Providers-Support]] for more details): target. Required element. Could be a table or updateable view source. Required element. Could be a table, query or client-side collection match/on rule. Optional element. Defines rule to match target and source records. By default we match target and source by primary key columns ordered list of operations to perform for each match. At least one operation required operation condition. Optional element. Specify additional condition for operation execution. Merge Operations Merge operations could be splitted into three groups: Matched operations. Operations, executed for records, present in both target and source according to match rule. Not matched operations. Operations, executed for records, present only in source according to match rule. Not matched by source. Operations, executed for records, present only in target according to match rule. Each group of operations work with their own set of source and target records and could contain more than one operation. In this case each operation must have operation condition except last one, which could omit it and be applied to all remaining records. Operations within group must be ordered properly. Example You want to do following: update status of all orders in AwaitingConfirmation status to Confirmed and delete all orders with amount equal to 0 . Your merge operation will look like: db.Orders // start merge command .Merge() // use the same table for source .UsingTarget() // match on primary key columns .OnTargetKey() // first delete all records with 0 amount // we also can use source in condition because they reference the same record in our case .DeleteWhenMatchedAnd((target, source) => target.amount == 0) // for records, not handled by previous command, update records in AwaitingConfirmation status .UpdateWhenMatchedAnd( (target, source) => target.status == Status.AwaitingConfirmation, (target, source) => new Order() { status = Status.Confirmed }) // send merge command to database .Merge(); In example above, delete and update operations belong to the same match group so their order is important. If you will put Update before Delete your merge command will do something else: it will update all orders in AwaitingConfirmation status and for remaining orders will remove those with 0 amount. After merge execution you could receive confirmed orders with 0 amount in Orders table. Matched operations Because those operations executed for records, present in both target and source, they have access to both records. There are two operations in this group (plus one non-standard operation for Oracle): Update operation. This operation allows to update target record fields. Delete operation. This operation allows to delete target record. Update Then Delete operation. This is Oracle-only operation, which updates target record and then delete updated records (usually using delete predicate). Not matched operations Those operations executed for records, present only in source table, so they could access only target table properties. This group contains only one operation - Insert operation, which adds new record to target table. Not matched by source operations This is SQL Server-only extension, that allows to perform operations for records, present only in target table. This group contains same operations as Matched group with one distinction - operations could access only target record: Update By Source operation. Allows to update target table record. Delete By Source operation. Allows to delete target table record."
  },
  "articles/sql/merge/Merge-API-Migration.html": {
    "href": "articles/sql/merge/Merge-API-Migration.html",
    "title": "Migrating from old Merge API to new | Linq To DB (aka linq2db)",
    "keywords": "Migrating from old Merge API to new This page contains information how to replace old Merge API calls with new API calls. Breaking changes Old API consider empty source list as noop operation and returns 0 without request to database. New version allways send command to database because: it will help to find errors in your command it will fix by source operations for SQL Server, which make sense for empty source Exception: Oracle, Sybase and SAP HANA implementations still use noop approach due to too aggressive type checking. Code migration Old API has 4x2 Merge methods. One method accepts target table as first parameter, another - DataConnection instance. New API works only with tables as target so you will need to get table from data connection using following code: dataConnection.GetTable<TTable>() If you used tableName , databaseName or schemaName parameters, replace them with follwing calls on table: db.GetTable<T>() .TableName(tableName) .DatabaseName(databaseName) .SchemaName(schemaName); Method 1 Parameters tableName , databaseName and schemaName omitted. // Old API int Merge<T>(this DataConnection dataConnection, IQueryable<T> source, Expression<Func<T,bool>> predicate); int Merge<T>(this ITable<T> table, IQueryable<T> source, Expression<Func<T,bool>> predicate); // New API // You can (and should) remove .AsEnumerable() - it was added to copy old behavior db.GetTable<T>() .Merge() .Using(source.Where(predicate).AsEnumerable()) .OnTargetKey() .UpdateWhenMatched() .InsertWhenNotMatched() .DeleteWhenNotMatchedBySourceAnd(predicate) .Merge(); Method 2 Parameters tableName , databaseName and schemaName omitted. // Old API int Merge<T>(this DataConnection dataConnection, Expression<Func<T,bool>> predicate, IEnumerable<T> source) int Merge<T>(this ITable<T> table, Expression<Func<T,bool>> predicate, IEnumerable<T> source); // New API db.GetTable<T>() .Merge() .Using(source) .OnTatgetKey() .UpdateWhenMatched() .InsertWhenNotMatched() .DeleteWhenNotMatchedBySourceAnd(predicate) .Merge(); Method 3 Parameters tableName , databaseName and schemaName omitted. // Old API int Merge<T>(this DataConnection dataConnection, bool delete, IEnumerable<T> source); int Merge<T>(this ITable<T> table, bool delete, IEnumerable<T> source); // New API // (delete = true) db.GetTable<T>() .Merge() .Using(source) .OnTargetKey() .UpdateWhenMatched() .InsertWhenNotMatched() .DeleteWhenNotMatchedBySource() .Merge(); // (delete = false) db.GetTable<T>() .Merge() .Using(source) .OnTargetKey() .UpdateWhenMatched() .InsertWhenNotMatched() .Merge(); Method 4 Parameters tableName , databaseName and schemaName omitted. // Old API int Merge<T>(this DataConnection dataConnection, IEnumerable<T> source); int Merge<T>(this ITable<T> table, IEnumerable<T> source); // New API db.GetTable<T>() .Merge() .Using(source) .OnTargetKey() .UpdateWhenMatched() .InsertWhenNotMatched() .Merge();"
  },
  "articles/sql/merge/Merge-API-Description.html": {
    "href": "articles/sql/merge/Merge-API-Description.html",
    "title": "Merge API Description | Linq To DB (aka linq2db)",
    "keywords": "Merge API Description Merge API contains four groups of methods: Merge , MergeInto , Using , UsingTarget methods to configure merge command's source and target On , OnTargetKey methods to configure merge command's match condition InsertWhenNotMatched* , UpdateWhenMatched* , DeleteWhenMatched* , UpdateWhenNotMatchedBySource* , DeleteWhenNotMatchedBySource* , UpdateWhenMatched*ThenDelete methods to add operations to merge command Merge and MergeAsync methods to execute command against database To create and execute merge command you should first configure target, source and match conditions. Then you must add at least one operation to merge builder. After that you should call Merge method to execute command. Note that all operation methods returns new merge builder, so code like that: // WRONG var db.Table.Merge().UsingTarget().OnTargetKey().DeleteWhenMatched(); // wrong, it will not modify merge object, but will create new one merge.InsertWhenNotMatched(); // execute merge with only one command - Delete merge.Merge(); // CORRECT db.Table.Merge().UsingTarget().OnTargetKey().DeleteWhenMatched().InsertWhenNotMatched().Merge(); General notes on API All API parameters are required and cannot be null. If you what to skip some parameter, check for a method without it. If there is no such method - this parameter cannot be ommited. Validation Before command execution, linq2db will try to validate your command and throw LinqToDBException if it detects use of feature, unsupported by provider or general misconfiguration. It will not detect all issues, but will greatly reduce number of errors from user side. Also validation error contains message that points to error in your command. Database engine errors sometimes require research to understand what they mean in current specific context. Operations API Merge operations will be added to generated query in the same order as they were called on command builder, because it is possible to specify several operations that could match the same record using operation conditions. In such cases database engine choose first matching operation as a winner. Also dont forget to check what your database engine could [[support|Merge-API-:-Background-Information-and-Providers-Support]] to understand what API you can use. Methods Target and Source Configuration Methods Match Configuration Methods InsertWhenNotMatched* UpdateWhenMatched* DeleteWhenMatched* UpdateWhenNotMatchedBySource* DeleteWhenNotMatchedBySource* UpdateWhenMatched*ThenDelete Merge and MergeAsync Target and Source Configuration Methods // starts merge command and use table parameter as target IMergeableUsing<TTarget> Merge<TTarget>(this ITable<TTarget> target); // adds source query to merge, started by Merge() method IMergeableOn<TTarget, TSource> Using<TTarget, TSource>(this IMergeableUsing<TTarget> merge, IQueryable<TSource> source); // adds source collection to merge, started by Merge() method IMergeableOn<TTarget, TSource> Using<TTarget, TSource>(this IMergeableUsing<TTarget> merge, IEnumerable<TSource> source); // adds target as source to merge, started by Merge() method IMergeableOn<TTarget, TTarget> UsingTarget<TTarget>(this IMergeableUsing<TTarget> merge); // starts merge command using source query and target table IMergeableOn<TTarget, TSource> MergeInto<TTarget, TSource>(this IQueryable<TSource> source, ITable<TTarget> target); Those methods allow you to create merge builder and specify source and target. To do it you can use: MergeInto method, which setups both source and target Merge + Using `UsingTarget` method sequence, where target and source specified by separate method. Methods could accept following parameters: target Target table, that should be modified by merge command. source Source data set, that should be merged into target table. Could be a client-side collection, table or query. Match Configuration Methods // adds match condition using specified key from target and source record // Examples: // merge.On(target => new { target.Field1, target.Field2 }, source => new { source.Field1, source.Field2 }) // merge.On(target => target.Id, source => source.Id) IMergeable<TTarget, TSource> On<TTarget, TSource, TKey>(this IMergeableOn<TTarget, TSource> merge, Expression<Func<TTarget, TKey>> targetKey, Expression<Func<TSource, TKey>> sourceKey); // add match condition using boolean expression over target and source record IMergeable<TTarget, TSource> On<TTarget, TSource>(this IMergeableOn<TTarget, TSource> merge, Expression<Func<TTarget, TSource, bool>> matchCondition); // adds match condition using primary key columns IMergeable<TTarget, TTarget> OnTargetKey<TTarget>(this IMergeableOn<TTarget, TTarget> merge); On `OnTargetKey` adds match condition to merge command builder. Notes matchCondition should be used only for rows matching. Any source filters must be applied to source directly to avoid database engine-specific side-effects (e.g. see Oracle limitations). matchCondition or match using keys shouldn't match more than one source record to one target record. InsertWhenNotMatched IMergeable<TTarget, TTarget> InsertWhenNotMatched<TTarget>(this IMergeableSource<TTarget, TTarget> merge); IMergeable<TTarget, TTarget> InsertWhenNotMatchedAnd<TTarget>(this IMergeableSource<TTarget, TTarget> merge, Expression<Func<TTarget, bool>> searchCondition); IMergeable<TTarget, TSource> InsertWhenNotMatched<TTarget, TSource>(this IMergeableSource<TTarget, TSource> merge, Expression<Func<TSource, TTarget>> setter); IMergeable<TTarget, TSource> InsertWhenNotMatchedAnd<TTarget, TSource>(this IMergeableSource<TTarget, TSource> merge, Expression<Func<TSource, bool>> searchCondition, Expression<Func<TSource, TTarget>> setter) InsertWhenNotMatched takes insert operation options and returns new merge command builder with new operation. InsertWhenNotMatchedAnd method additionally takes operation condition expression. merge Merge command builder. Method will return new builder with new insert operation. It will not modify original object. searchCondition Operation execution condition. Operation without condition will be applied to all matching records. If there are multiple operations within same group - only last one allowed to have no condition. WhenNotMatched match group could contain only Insert operations. setter Record creation expression. Defines set ex InsertWhenNotMatched takes insert operation options and returns new merge command builder with new operation. InsertWhenNotMatchedAnd method additionally takes operation condition expression. pressions for values in new record. For methods without this parameters source record inserted into target (except fields marked with SkipOnInsert attribute or IsIdentity for provider without identity insert support). db.Table .Merge() .Using(source) .OnTargetKey() .InsertWhenNotMatched(source => new TargetRecord() { Field1 = 10, Field2 = source.Field2, Field3 = source.Field1 + source.Field2 }) .Merge(); UpdateWhenMatched IMergeable<TTarget, TTarget> UpdateWhenMatched<TTarget>(this IMergeableSource<TTarget, TTarget> merge); IMergeable<TTarget, TTarget> UpdateWhenMatchedAnd<TTarget>(this IMergeableSource<TTarget, TTarget> merge, Expression<Func<TTarget, TTarget, bool>> searchCondition) IMergeable<TTarget, TSource> UpdateWhenMatched<TTarget, TSource>(this IMergeableSource<TTarget, TSource> merge, Expression<Func<TTarget, TSource, TTarget>> setter); IMergeable<TTarget, TSource> UpdateWhenMatchedAnd<TTarget, TSource>(this IMergeableSource<TTarget, TSource> merge, Expression<Func<TTarget, TSource, bool>> searchCondition, Expression<Func<TTarget, TSource, TTarget>> setter); UpdateWhenMatched takes update operation options and returns new merge command builder with new operation. UpdateWhenMatchedAnd method additionally takes operation condition expression. merge Merge command builder. UpdateWhenMatched method will return new builder with new update operation. It will not modify original object. searchCondition Operation execution condition. Operation without condition will be applied to all matching records. If there are multiple operations within same group - only last one could omit condition. WhenMatched match group could contain only Update and Delete operations. setter Record update expression. Defines update expressions for values in target record. When not specified, source record values used to update target record (except fields marked with SkipOnUpdate or IsIdentity attributes). db.Table .Merge() .Using(source) .OnTargetKey() .UpdateWhenMatched((target, source) => new TargetRecord() { Field1 = target.Field10, Field2 = source.Field2, Field3 = source.Field1 + target.Field2 }) .Merge(); DeleteWhenMatched IMergeable<TTarget, TSource> DeleteWhenMatched<TTarget, TSource>(this IMergeableSource<TTarget, TSource> merge); IMergeable<TTarget, TSource> DeleteWhenMatchedAnd<TTarget, TSource>(this IMergeableSource<TTarget, TSource> merge, Expression<Func<TTarget, TSource, bool>> searchCondition); DeleteWhenMatched takes delete operation options and returns new merge command builder with new operation. merge Merge command builder. DeleteWhenMatched method will return new builder with new delete operation. It will not modify original object. searchCondition Operation execution condition. Operation without condition will be applied to all matching records. If there are multiple operations within same match group - only last one could omit condition. WhenMatched match group could contain only Update and Delete operations. UpdateWhenNotMatchedBySource IMergeable<TTarget, TSource> UpdateWhenNotMatchedBySource<TTarget, TSource>(this IMergeableSource<TTarget, TSource> merge, Expression<Func<TTarget, TTarget>> setter); IMergeable<TTarget, TSource> UpdateWhenNotMatchedBySourceAnd<TTarget, TSource>(this IMergeableSource<TTarget, TSource> merge, Expression<Func<TTarget, bool>> searchCondition, Expression<Func<TTarget, TTarget>> setter); IMPORTANT: This method could be used only with SQL Server. UpdateWhenNotMatchedBySource takes update operation options and returns new merge command builder with new operation. merge Merge command builder. UpdateWhenNotMatchedBySource method will return new builder with new update operation. It will not modify original object. searchCondition Operation execution condition. Operation without condition will be applied to all matching records. If there are multiple operations within same group - only last one could omit condition. WhenNotMatchedBySource match group could contain only UpdateWhenNotMatchedBySource and DeleteWhenNotMatchedBySource operations. But due to SQL Server limitations you can use only one UpdateWhenNotMatchedBySource and DeleteWhenNotMatchedBySource operation in single command. setter Record update expression. Defines update expressions for values in target record. db.Table .Merge() .Using(source) .OnTargetKey() .UpdateWhenNotMatchedBySource(target => new TargetRecord() { Field1 = target.Field10, Field2 = target.Field2, Field3 = target.Field3 + 10 }) .Merge(); DeleteWhenNotMatchedBySource IMergeable<TTarget, TSource> DeleteWhenNotMatchedBySource<TTarget, TSource>(this IMergeableSource<TTarget, TSource> merge); IMergeable<TTarget, TSource> DeleteWhenNotMatchedBySourceAnd<TTarget, TSource>(this IMergeableSource<TTarget, TSource> merge, Expression<Func<TTarget, bool>> searchCondition); IMPORTANT: This method could be used only with SQL Server. DeleteWhenNotMatchedBySource takes delete operation options and returns new merge command builder with new operation. merge Merge command builder. DeleteWhenNotMatchedBySource method will return new builder with new delete operation. It will not modify original object. searchCondition Operation execution condition. Operation without condition will be applied to all matching records. If there are multiple operations within same group - only last one could omit condition. WhenNotMatchedBySource match group could contain only UpdateWhenNotMatchedBySource and DeleteWhenNotMatchedBySource operations. But due to SQL Server limitations you can use only one UpdateWhenNotMatchedBySource and DeleteWhenNotMatchedBySource operation in single command. UpdateWhenMatchedThenDelete IMergeable<TTarget, TTarget> UpdateWhenMatchedThenDelete<TTarget>(this IMergeableSource<TTarget, TTarget> merge, Expression<Func<TTarget, TTarget, bool>> deleteCondition); IMergeable<TTarget, TTarget> UpdateWhenMatchedAndThenDelete<TTarget>(this IMergeableSource<TTarget, TTarget> merge, Expression<Func<TTarget, TTarget, bool>> searchCondition, Expression<Func<TTarget, TTarget, bool>> deleteCondition); IMergeable<TTarget, TSource> UpdateWhenMatchedThenDelete<TTarget, TSource>(this IMergeableSource<TTarget, TSource> merge, Expression<Func<TTarget, TSource, TTarget>> setter, Expression<Func<TTarget, TSource, bool>> deleteCondition); IMergeable<TTarget, TSource> UpdateWhenMatchedAndThenDelete<TTarget, TSource>(this IMergeableSource<TTarget, TSource> merge, Expression<Func<TTarget, TSource, bool>> searchCondition, Expression<Func<TTarget, TSource, TTarget>> setter, Expression<Func<TTarget, TSource, bool>> deleteCondition); IMPORTANT: This method could be used only with Oracle Database. UpdateWhenMatchedThenDelete method takes update and delete operation options and returns new merge command builder with new operation. merge Merge command builder. UpdateWhenMatchedThenDelete method will return new builder with new update with delete operation. It will not modify original object. searchCondition Update operation execution condition. Operation without condition will be applied to all matching records. Oracle doesn't support multiple commands in current match group. You can use only UpdateWhenMatchedThenDelete or UpdateWhenMatched in single command. setter Record update expression. Optional. Defines update expressions for values in target record. db.Table .Merge() .From(source) .OnTargetKey() .UpdateWhenMatchedThenDelete((target, source) => new TargetRecord() { Field1 = target.Field10, Field2 = source.Field2, Field3 = source.Field1 + target.Field2 }, (updatedTarget, source) => updatedTarget.Field3 > 100) .Merge(); deleteCondition Delete operation execution condition. Identifies updated records that should be deleted. Note that this condition applied to updated target record with new field values. Merge int Merge<TTarget, TSource>(this IMergeable<TTarget, TSource> merge); Task<int> MergeAsync<TTarget, TSource>(this IMergeable<TTarget, TSource> merge, CancellationToken token = default); Merge method builds and executes merge command against database and returns number of affected records. MergeAsync does the same job asynchronously. merge Merge command builder. Notes Merge returns number of affected records. Consult your database documentation for more details, but in general except SAP/Sybase ASE it is the same for all databases."
  },
  "articles/sql/merge/Merge-API-Background.html": {
    "href": "articles/sql/merge/Merge-API-Background.html",
    "title": "Merge API Background Information | Linq To DB (aka linq2db)",
    "keywords": "Merge API Background Information Merge API uses MERGE INTO command defined by SQL:2003 standard with updates in SQL:2008 . Additionally we support some non-standard extensions to this command. See specific database engine support information below. Later we plan to extend providers support by adding support for UPSERT -like commands. Basic syntax (SQL:2003) MERGE INTO <target_table> [[AS] <alias>] USING <source_data_set> [[AS] <alias>] ON <match_condition> -* one or both cases could be specified WHEN MATCHED THEN <update_operation> WHEN NOT MATCHED THEN <insert_operation> <update_operation> := UPDATE SET <column> = <value> [, <column> = <value>] <insert_operation> := INSERT (<column_list>) VALUES(<values_list>) Advanced syntax (SQL:2008 extensions) Multiple MATCH cases It is possible to perform different operations for records, matched by ON match condition by specifying extra conditions on WHEN statement: WHEN [NOT] MATCHED [AND <extra_condition>] THEN <match_specific_operation> DELETE operation DELETE operation could be used for WHEN MATCHED case. WHEN MATCHED [AND <extra condition>] THEN DELETE Links MERGE on wikibooks SQL grammar see SQL:2003 and SQL:2011 (sic! grammars) Supported Databases Microsoft SQL Server IBM DB2 Firebird Oracle Database Sybase/SAP ASE IBM Informix SAP HANA 2 General considerations Not all data types supported or have limited support for some providers right now if you use client-side source. Usually it will be binary types. Check notes for specific provider below. Microsoft SQL Server 2008+ Microsoft SQL Server supports Merge API starting from SQL Server 2008 release. It supports all features from SQL:2008 standard and adds support for two new operations, not available for other providers: Update by source operation Delete by source operation Those two operations allow to update or delete target record when no matching record found in source. Of course it means that only target record available in context of those two operations. Limitations: operation of each type can be used only once in merge command even with different predicates only up to three operations supported in single command Other notes: identity insert enabled for insert operation Links: MERGE INTO command IBM DB2 Note: merge implementation was tested only on DB2 LUW. DB2 supports all features from SQL:2008 standard. Limitations: doesn't support associations (joins) in match predicate Links: MERGE INTO DB2 z/OS 12 MERGE INTO DB2 iSeries 7.3 MERGE INTO DB2 LUW 11.1 Firebird Firebird 2.1-2.5 supports all features from SQL:2003 standard. Firebird 3.0 supports all features from SQL:2008 standard. Limitations: update of fields, used in match condition could lead to unexpected results in Firebird 2.5 very small double values in client-side source could fail BLOB and TIMESTAMP mapped to TimeSpan will not work with client-side source if null values mixed with non-null values Links: Firebird 2.5 MERGE INTO Firebird 3.0 MERGE INTO (PDF;Russian) Oracle Database Oracle supports SQL:2003 features and operation conditions from SQL:2008 . Instead of independent Delete operation it supports delete condition for Update operation, which will be applied only to updated records and work with updated values. To support this behavior, merge API supports Update Then Delete operation, that works only for Oracle. You also can use regular Update operation, but not Delete . For Delete operation you can use `UpdateWithDelete' with the same condition for update and delete. Limitations: Only two operations per command supported, where one of them should be Insert and second should be Update or UpdateWithDelete Delete operation not supported Associations in `Insert' setters not supported fields, used in match condition, cannot be updated command with empty enumerable source will not send command to database and return 0 immediately mixing nulls and non-null values for binary column for client-side source doesn't work Links: MERGE INTO Sybase/SAP ASE ASE supports all features from SQL:2008 standard Limitations: it is hard to name it just a limitation * server could crash on some merge queries associations in match condition not supported (undocumented) returned number of affected records could be (and usually is) more than expected Merge only with Delete operations doesn't work (undocumented) Some combinations of operations rise error with text that doesn't make any sense (undocumented): \" MERGE is not allowed because different MERGE actions are referenced in the same WHEN [NOT] MATCHED clause \", which is not true, because other commands with same set of operations just work command with empty enumerable source will not send command to database and return 0 immediately Other notes: identity insert enabled for insert operation Links: MERGE INTO ASE 15.7 MERGE INTO ASE 16 IBM Informix Informix supports all features from SQL:2003 standard and Delete operation from SQL:2008 . Limitations: associations not supported BYTE type (C# byte[] binary type) in client-side source leads to unexpected results for unknown reason Other notes: for enumerable source it could be required to specify database types on columns that contain null values if provider cannot infer them properly Links: MERGE INTO SAP HANA 2 SAP HANA 2 supports all features from SQL:2003 standard. Limitations: Update operation must be first if both Update and Insert operations used in command associations in Insert operation not supported command with empty enumerable source will not send command to database and return 0 immediately Links: MERGE INTO"
  },
  "articles/releasenotes/2.0.0.html": {
    "href": "articles/releasenotes/2.0.0.html",
    "title": "v2.0.0 Release notes | Linq To DB (aka linq2db)",
    "keywords": "v2.0.0 Release notes General Changes Breaking Changes CTE Support Mapping MappingSchema.EntityDescriptorCreatedCallback Fluent Mapping Dynamic Columns Calculated Columns Inheritance Mapping Other Changes and Fixes SQL Generation Extensions Temp Tables API Bulk Copy Merge Schema Provider Other Changes and Fixes Provider-specific changes Access DB2 Firebird Informix MySQL and MariaDB Oracle PostgreSQL SAP HANA SQL CE Sybase/SAP ASE SQLite SQL Server Changes for developers I Use Entity Framework General changes Also check provider-specific changes for your provider as this section contains only provider-independent changes. Breaking Changes Changes to target frameworks Version 2.0 drops support for legacy frameworks: net4.0, silverlight, windows8 store. List of supported targets now includes: net45 netstandard1.6 netstandard2.0 netcoreapp2.0 Also you can notice that now linq2db nuget package supports all those targets and you don't need to use linq2db.core package if you want to target .net core projects. This package is now deprecated and will not be updated anymore. Who will be affected by this change: People that used deprecated frameworks. They should continue use 1.x version or migrate their projects. Users of linq2db.core package. They should update their nuget references to use linq2db package. Default enumeration mapping behavior changes for text columns Starting from version 2.0, LINQ To DB will use ToString() method to create database value for enums, mapped to text column ( 1006 , 1071 ). Prior versions used numeric representation of enumeration value, converted to string. How to find out if I'm affected by this change? You are affected if you have text columns mapped to enumeration without explicit mappings specified for fields. See following example: // enums like that are not affected, because they // have explicit mappings from enum fields to database values public enum GoodEnum { [MapValue(\"1\")] First, [MapValue(\"Second\")] Second } // enums like that will change their behavior public enum BadEnum { // v1.x: \"0\" used as a database value // v2: \"First\" used as a database value First, // v1.x: \"1\" used as a database value // v2: \"Second\" used as a database value Second } We want to say that it is generally bad idea to have no explicit enumeration field mappings as you depend on library behavior, which could change, and changes to enumeration (adding/removing/reordering fields) could lead to mapped value change. I'm affected. What should I do? First of all you can re-enable old behavior using following configuration flag: Configuration.UseEnumValueNameForStringColumns = false; But we recommend to add explicit mappings to your enumeration. CTE Support This release adds native support for common table expressions, including recursive CTE ( 534 , 890 ). You can read more about this feature here . Mapping MappingSchema.EntityDescriptorCreatedCallback This new callback could be used to modify entity mapping descriptor after creation (1074) . E.g. you can use it to change columns name notation to snake-case. ms.EntityDescriptorCreatedCallback = (mappingSchema, entityDescriptor) => { // let's imagine we have ToSnakeCase string // extension method somewhere in our project entityDescriptor.TableName = entityDescriptor.TableName.ToSnakeCase(); foreach (var entityDescriptorColumn in entityDescriptor.Columns) { entityDescriptorColumn.ColumnName = entityDescriptorColumn.ColumnName.ToSnakeCase(); } }; Fluent Mapping Custom join predicate exression support for associations 961 fluentBuilder .Entity<Entity>() .Association( e => e.AssociationProperty, (thisSide, otherSide) => thisSide.Id == otherSide.ID1); Other Changes and Fixes complex types mapping fixed (1005) Dynamic Columns This new feature will allow you to use dynamic columns in your queries ( 507 , 744 , 964 , 1083 ). Check this PR for more details. Calculated Columns You can use expressions to define calculated columns using IsColumn property of ExpressionMethodAttribute attribute (1004) . [Table] public class Entity { // normal read/write columns [Column] public string FirstName { get; set; } [Column] public string LastName { get; set; } // read-only expression-based property [ExpressionMethod(nameof(FullNameExpr), IsColumn = true)] public string FullName { get; set; } private static Expression<Fun<Entity, string>> FullNameExpr() { return e => e.LastName + \", \" + e,FirstName; } } Inheritance Mapping fixed incorrect query filter generation for left join associations for entities with inheritance mapping (956) fixed exception when inherited entity selected into property of base type (1046) Update / Delete / Insert / InsertOrReplace extensions will properly recognize inherited values when passed as parameter of base type. Important: entities without inheritance mapping not affected by this change and query generation will use parameter type as before (1017) LoadWith fixed to properly load derived entities (994) fixed several issues with type conversion in expressions between base and derived types ( 1057 , 1065 ) fixed exception when LoadWith called for nullable reference to entity with inheritance mapping (996) Other Changes and Fixes fixed MappingSchema converters were ignored for enums (1006) fixed issue when adding new metadata reader to MappingSchema.Default could result in previously added readers being ignored (1066) default char mapping will now use Length = 1 (1091) improved support of interfaces (1099) SQL Generation CROSS/OUTER APPLY support For databases that support APPLY joins you can disable this functionality using following flag: // set to true by default // v2.0-beta5 has typo in property name : PrefereApply Configuration.PreferApply = false; Other Changes and Fixes fixed exception generated for some cases when joins optimization enabled and table hints used (949) fixed SQL generation regression for some complex subqueries (928) fixed invalid SQL generated for empty select combined with Take / Skip (817) fixed issue when joined subquery condition were moved to outer query condition (922) Extensions New In / NotIn extension methods SqlExtensions class contains new extension methods In / NotIn , applied to a value. In general it is just a reverse Contains methods to better mimic SQL. // filter table by id // old reverse logic approach db.Table.Where(r => ids.Contains(r.Id)); // now it could be written like you do it in raw SQL db.Table.Where(r => r.Id.In(ids)); New *Join extension methods ( 1076 , 1088 ) Additional extension methods added to define join using two queryable sources, join predicate expression and result selector. Following methods added: InnerJoin , LeftJoin , RightJoin , FullJoin and CrossJoin . // left outer join db.Parent.LeftJoin( db.Child, (p, c) => p.ParentID == c.ParentID, (p, c) => new { ParentID = p.ParentID, ChildID = (int?)c.ChildID }); // cross join - note that join condition is not applicable here db.Parent.CrossJoin( db.Child, (p, c) => new { ParentID = (int?)p.ParentID, ChildID = (int?)c.ChildID }); Added missing support for table, schema/owner and database name parameters in some methods Those optional parameters now available also for following IDataContext extension methods: InsertOrReplace* InsertWith*Identity* Update* Delete* Parameters creation in extension builders You can add new query parameters from extension builders (973) . class InWithParametersBuilder : Sql.IExtensionCallBuilder { public void Build(Sql.ISqExtensionBuilder builder) { // get extension parameter var values = builder.GetValue<System.Collections.IEnumerable>(\"values\"); // tell linq2db that query uses non-static parameters builder.Query.IsParameterDependent = true; foreach (var value in values) { // create query parameter var param = new SqlParameter(value?.GetType() ?? typeof(object), \"p\", value); // add parameter (note that we can add multiple parameters for one placeholder) builder.AddParameter(\"values\", param); } } } public static class Extensions { // note that for values we specify comma as parameters delimiter [Sql.Extension(\"{field} IN ({values, ', '})\", IsPredicate = true, BuilderType = typeof(InWithParametersBuilder))] public static bool In<T>(this Sql.ISqlExtension ext, [ExprParameter] T field, params T[] values) { throw new NotImplementedException(); } } New extensibility points LinqExtensions.ProcessSourceQueryable delegate You can use it to preprocess IQueryable sources, passed to other APIs (1116) . E.g. it is used by our other project https://github.com/linq2db/linq2db.EntityFrameworkCore to replace EntityFramework queryable provider with LINQ To DB provider. LinqExtensions.ExtensionsAdapter interface You can override implementation of LINQ To DB async methods using this interface. Note that right now it is all-or-nothing interface, so you need to implement all methods you use, even if you want to change behavior of only one method. In this case you need to call original LINQ To DB method for others. IEntityServices.OnEntityCreated delegate This delegate will be called after entity instantiation for contexts that implement IEntityServices interface (1112) . It allows you to pre-process or even replace created entity. LINQ To DB already implements this interface for default contexts: DataContext , DataConnection and RemoteDataContextBase classes. using (var db = new DataConnection()) { var cnt = 0; // just count how many User entities were instantiated db.OnEntityCreated += e => { if (e.Entity is User) cnt++; } // do some queries } DataContext.OnTraceConnection callback Added trace callback for DataContext class similar to one that already exist on DataConnection class (1131) . Temp Tables API New API to create temporary tables added to IDataContext as a set of CreateTempTable() extension methods. Using this API you can create queryable table, populate it with data, perform queries and then delete by disposing it. Example below shows you how can you use merge with client-side source in more effective way by adding those records into temp table and merge them into main table. public void MergePersons(this IDataContext db, IEnumerable<Person> persons) { // create new table for existing Person mapping and populate it using bulk copy using (var tmp = db.CreateTempTable<Person>(persons, tableName: \"PersonTemp\")) { db.Persons // target table .Merge() .Using(tmp) // use data from temp table .OnTargetKey() .InsertWhenNotMatched() // insert new records .UpdateWhenMatched() // update known records .DeleteWhenNotMatchedBySource() // delete others .Merge(); } // here dispose will delete temp table } API allows you: specify new table, owner/schema, database name using corresponding parameters populate table using data from IQueryable source parameter populate table using bulk copy from IEnumerable source parameter and BulkCopyOptions adjust entity mapping for temporary table using fluent mapping delegate using setTable parameter call some action on created table before populating it with data using action parameter Bulk Copy Documentation New article created for BulkCopy API. It still miss documentation on some options, but most important information is already here. It will be improved in next releases. KeepIdentity option KeepIdentity option were documented and existing implementations tested/fixed to follow it (1037) . Setting this option to true for RowByRow copy mode was never supported and if you have it set to true - starting from version 2.0 it will start throwing exception instead of silently ignore it. Check copy mode support table to see what mode actually used for your provider, as BulkCopy will downgrade copy mode, if requested one is not supported by provider. Merge Partial projections in source query improvements Now merge will properly detect and throw exception when source query element type contains more fields than query returns. class Person { [PrimaryKey, Column, Identity] public int Id { get; set; } [Column] public string FirstName { get; set; } [Column] public string LastName { get; set; } [Column] public string Title { get; set; } } // bad query db.Persons.Merge() // note that Id and Title columns is not selected by query .Using(db.NewPersons.Select(p => new Person() { FirstName = p.LastName, LastName = p.FirstName })) // will throw exception that Id key field missing in source .OnTargetKey() // will throw exception that Title field missing in source .InsertWhenNotMatched(s => new Person() { Title = s.Title }) .Merge(); Other Changes and Fixes CROSS JOIN and SelectMany support in source query (896) Merge call will not be available anymore if you didn't specified any operations yet fixed exception of empty local source with source type != target type (1153) Schema Provider Requesting schema for procedures and functions will wrap it internally in transaction with rollback, when called without transaction. This is done to avoid situations when some providers execute (sic!) procedures instead of just returning their schema. Taking into account such bugs it is recommended to never wrap schema provider calls into transaction and let LINQ To DB handle it. Other Changes and Fixes errors during mapping of data from database to mapping class field on selects will now be wrapped into LinqToDBException with details what field failed with original error in InnerException (1065) fixed issue when selected of NULL value using SelectMany method could have resulted in default(T) value for value types even with cast to T? (1012) fixed exception passing binary data over WCF (925) spelling error fixed for SchemaProvider.ForeignKeyInfo (941) enforce server-side evaluation of Sql.Lower / Sql.Upper functions (819) fixed case when async code could be blocked on synchronous Connection.Open call (1023) fixed support for type casts in LoadWith expression (1069) fixed issue when insert query with sub-query data source will fail on next calls if it has nullable parameter and first call uses null for parameter value (1098) T4.Models repository was obsolete project and moved to linq2db repo removed use of database object owner name from many APIs to reduce confusion with having owner and schema overrides at the same time, meaning the same concept T4 templates support in .NET Core projects (1067) new DataConnection.GetRegisteredProviders method to return list of all registered data providers Provider-specific changes Access handle exceptions from OleDb provider on schema read calls when ACE provider used (10) schema provider will now return system tables too ( TableInfo.IsProviderSpecific == true ) (1119) DB2 IBM.Data.DB2.Core provider support - .NET Core DB2 provider support was added. schema provider doesn't return procedures and functions if GetTables = false specified (1068) Firebird Default identifier quotation mode change Default identifier quotation mode changed from FirebirdIdentifierQuoteMode.None to FirebirdIdentifierQuoteMode.Auto (1120) . Normally it shouldn't affect anybody, as Auto mode will quote only invalid and reserved identifiers and they will not work anyway in None mode. Still, technically it could be a case for people who used quoted identifier in mappings. This is not something you should do, as mapping should contain raw identifers and quotation should be handled by LINQ To DB. If you did that we want to hear why you need to do it and how we can improve LINQ To DB in this area. If you are affected by this change, just restore old quotation mode using following code: FirebirdSqlBuilder.IdentifierQuoteMode = FirebirdIdentifierQuoteMode.None; Other changes and fixes BulkCopy will throw exception if KeepIdentity = true option specified as this option is not supported for Firebird (1037) . Check BulkCopy documentation for more details DropTable API will check if dropped objects exist before dropping them (1120) fixed support of seconds and milliseconds by Sql.DatePart function (967) detect and escape identifiers that use reserved words in FirebirdIdentifierQuoteMode.Auto mode ( 1095 , 1110 ) CreateTable / DropTable / TruncateTable will respect identifier quotation mode during query generation (1120) FirebirdDataProvider and FirebirdSqlOptimizer classes made public to help users override default implementation (1000) Informix Added delimited identifiers support MySQL and MariaDB Procedures and functions support by schema provider Schema provider for MySQL/MariaDB was updated to return procedures and functions (991) . Requesting procedures and functions from transaction will throw exception because schema provider need to wrap it internally into transaction to avoid procedure execution due to bug in provider (792) Oracle .NET Core provider support Support for beta version of .NET Core provider added. Other changes and fixes date literal generation fixes (969) schema provider doesn't return procedures and functions if GetTables = false specified (1068) detect and escape identifiers that use reserved words ( 1095 , 1110 ) PostgreSQL Native bulk copy support Version 2.0 adds support for native bulk copy method (935) . It is available through existing BulkCopy LINQ To DB API by specifying BulkCopyOptions.BulkCopyType = BulkCopyType.ProviderSpecific . It is a high-level wrapper over COPY command. Note that if you already used this mode for your bulk copy operations it could be a breaking change because now it will use COPY instead of silent fallback to BulkCopyType.MultipleRows in previous versions. Why it could be a breaking change? Because COPY command (we use BINARY mode) demands that proper column types specified and will fail if types doesn't match. You will need to add type information to your mappings or switch to other copy method. You can read about type requirements more in out new article about BulkCopy API here . Implementation supports COPY API from both npgsql 3.x and npgsql 4.x (4.0 brings breaking changes to API). Upsert support InsertOrUpdate API will use INSERT ON CONFLICT UPDATE statement for PostgreSQL 9.5+ instead of several statements on previous versions (948) . You will need to use PostgreSQLVersion.v95 provider version if you don't use version autodetect. Other changes and fixes improved support for some database types as a part of BulkCopy improvements (1091) fixed support for following types in CreateTable API: Int16 / Int64 identity columns, System.Linq.Binary , DataType.VarBinary , DataType.NChar(1) , char (1091) support for interval type mapping to both NpgsqlTimeSpan and NpgsqlInterval types (1091) SAP HANA BulkCopy KeepIdentity option support As a part of KeepIdentity option review, support for it added to SAP HANA provider. Check documentation for more details (1037) . Note that this option requires support from provider, so make sure you use recent provider version with enum HanaBulkCopyOptions having KeepIdentity field and not all versions had it. Otherwise BulkCopy will throw exception. SQL CE BulkCopy KeepIdentity option support As a part of KeepIdentity option review, support for it added to SQL CE provider. Check documentation for more details (1037) . Sybase MERGE insert operation will respect SkipOnInsert on identity fields when InsertWhenNotMatched() without custom setter used and allow database to generate field's value (914) requesting procedures and functions from schema provider will throw exception if called from transaction to avoid database corruction due to bug in provider (792) SQLite DateTime.AddDays() method to SQL conversion fixed (998) SQL Server varchar / nvarchar parameters will use 8000/4000 as length to improve query plans caching (989) MERGE will use parameters instead of literals for binary data in client-side (IEnumerable) source MERGE insert operation will respect SkipOnInsert on identity fields when InsertWhenNotMatched() without custom setter used and allow database to generate field's value (914) legacy MERGE API will not try to update identity columns anymore on update operation anymore (1007) fixed DropTable method not dropping table in another database (1030) fixed DateTime literal generation (1107) fixed incompatibility between BulkCopy and RetryingDbConnection (1135) Changes for developers Query AST was refactored. See SelectQuery class ( 936 , 938 ) Tests configuration changed format and use *DataProviders.json files instead of *DataPRoviders.txt. More details Project migrated to support latest C# version You can use new ActiveIssueAttribute to mark tests for non-fixed issues. This will allow to merge test-only PRs immediately SchemaProviderBase methods ToTypeName and ToValidName made public ( 944 , 963 ) SqlProviderFlags.CustomFlags list added to allow store custom provider flags (1154) expose SqlExtensions class for provider developers I Use Entity Framework (:feelsbadman:) Don't worry, check this new project we created recently. It is still an early prototype so don't expect it to work flawlessly. We will appreciate your feedback!"
  },
  "articles/get-started/install/index.html": {
    "href": "articles/get-started/install/index.html",
    "title": "Installing LINQ To DB | Linq To DB (aka linq2db)",
    "keywords": "Installing LINQ To DB Visual Studio development You can develop many different types of applications that target .NET Core, .NET Framework, or other platforms supported by LINQ To DB using Visual Studio. There are two ways you can install the LINQ To DB database provider in your application from Visual Studio: Using NuGet's Package Manager User Interface Select on the menu Project > Manage NuGet Packages Click on the Browse or the Updates tab Select the linq2db.SqlServer package and the desired version and confirm (see list of supported databases ) Tip linq2db package contains all Data Providers in bundle and loads their libraries dynamically. For simplyfying usage, LINQ To DB has many helper packages linq2db.* that reference required libraries and provide T4 Templates for particular Data Provider. Using NuGet's Package Manager Console (PMC) Select on the menu Tools > NuGet Package Manager > Package Manager Console Type and run the following command in the PMC: Install-Package linq2db.SqlServer You can use the Update-Package command instead to update a package that is already installed to a more recent version To specify a specific version, you can use the -Version modifier. For example, to install LINQ To DB packages, append -Version 2.1.0 to the commands"
  },
  "articles/get-started/full-dotnet/index.html": {
    "href": "articles/get-started/full-dotnet/index.html",
    "title": "Getting Started on .NET Framework - LINQ To DB | Linq To DB (aka linq2db)",
    "keywords": "Getting Started with LINQ To DB on .NET Framework These tutorials require no previous knowledge of LINQ To DB or Visual Studio. They will take you step-by-step through creating a simple .NET Framework Console Application that queries and saves data from a database. You can use the techniques learned in these tutorials in any application that targets the .NET Framework, including WPF and WinForms. Note These tutorials and the accompanying samples have been updated to use LINQ To DB 2.1.0. However, in the majority of cases it should be possible to create applications that use previous releases, with minimal modification to the instructions."
  },
  "articles/get-started/full-dotnet/existing-db.html": {
    "href": "articles/get-started/full-dotnet/existing-db.html",
    "title": "Getting Started on .NET Framework - Existing Database - LINQ To DB | Linq To DB (aka linq2db)",
    "keywords": "Getting started with LINQ To DB on .NET Framework with a Existing Database In this walkthrough, you will build a console application that performs basic data access against a Microsoft SQL Server database using LINQ To DB. You will use existing database to create your model. Tip You can view this article's sample on GitHub. Prerequisites The following prerequisites are needed to complete this walkthrough: Visual Studio 2017 Latest version of NuGet Package Manager Latest version of Windows PowerShell Northwind Database Tip Also there is Northwind Database script file which can be executed using Microsoft SQL Server Management Studio Create a new project Open Visual Studio File > New > Project... From the left menu select Templates > Visual C# > Windows Classic Desktop Select the Console App (.NET Framework) project template Ensure you are targeting .NET Framework 4.5.1 or later Give the project a name and click OK Install LINQ To DB To use LINQ To DB, install the package for the database provider(s) you want to target. This walkthrough uses SQL Server. For a list of available providers see Database Providers . Tools > NuGet Package Manager > Package Manager Console Run Install-Package linq2db.SqlServer Generate model from database Now it's time to generate your model from database. Project > New Folder... Enter DataModels as the name and click OK DataModels > Add Item... and select Text Template Enter Northwind.tt as the name and click OK Copy the content from the file Project > LinqToDB.Templates\\CopyMe.SqlServer.tt.txt Modify host, database name and credentials for your SQL Server in LoadSqlServerMetadata function call Save Northwind.tt - it should invoke model generation Tip There are many ways to customize generation process. Follow this link for details. Use your model You can now use your model to perform data access. Open App.config Replace the contents of the file with the following XML (correct connection string based on your server location and credentials) <!-- [!code-xml[Config](https://github.com/linq2db/examples/blob/master/SqlServer/GetStarted/App.config)] --> <?xml version=\"1.0\" encoding=\"utf-8\"?> <configuration> <connectionStrings> <add name=\"MyDatabase\" providerName=\"System.Data.SqlClient\" connectionString=\"Data Source=.;Database=Northwind;Integrated Security=SSPI;\" /> </connectionStrings> </configuration> Open Program.cs Replace the contents of the file with the following code <!-- [!code-csharp[Main](https://github.com/linq2db/examples/blob/master/SqlServer/GetStarted/Program.cs)] --> using System; using System.Linq; namespace GetStarted { class Program { static void Main(string[] args) { using (var db = new DataModel.NorthwindDB()) { var q = from c in db.Customers select c; foreach (var c in q) Console.WriteLine(c.ContactName); } } } } Debug > Start Without Debugging You will see list of Contact names."
  },
  "articles/general/supportedfw.html": {
    "href": "articles/general/supportedfw.html",
    "title": "Supported frameworks | Linq To DB (aka linq2db)",
    "keywords": "Supported frameworks Framework linq2db linq2db.Core .Net 4 Yes Yes .Net 4.5 Yes Yes .Net 4.5.1 Yes Silverlight 4+ Yes Mono Yes WindowsStore 8 Yes .NETStandard 1.6 Yes"
  },
  "articles/general/Managing-data-connection.html": {
    "href": "articles/general/Managing-data-connection.html",
    "title": "Managing data connection | Linq To DB (aka linq2db)",
    "keywords": "Managing data connection .NET database providers use connection pooling to work with database connections, where they take connection from pool, use it, and then release connection back to connection pool so it could be reused. When connection is not released correctly after use, connection pool will consider it still used, which will lead to two consequences: your application will create more and more connections to database, because there are no free connections to reuse from connection pool manager point of view at some point your application will fail to obtain connection from pool, because pool size limit reached To avoid collection leaks you should care about how you are creating and disposing connections. There are to ways to query database with linq2db: using DataConnection class. Using DataConnection you can make several queries in one physical database connection, so you do not have overhead on opening and closing database connection. You should follow few simple rules: always dispose DataConnection instance (it is recommended to use using c# statement); query should be executed before DataConnection object is disposed. From version 1.8.0 we have introduced protection from wrong usage, and you will get ObjectDisposedException trying to perform query on disposed DataConnection instance. using DataContext class. DataContext opens and closes physical connection for each query! Be careful with DataContext.KeepConnectionAlive property, if you set it true , it would work the same way as DataConnection ! So we do not recommend you to set this property to true . Done right using (var db = new DataConnection()) { // your code here } public IEnumerable<Person> GetPersons() { using (var db = new DataConnection()) { // ToList call sends query to database while we are still in using return db.GetTable<Person>().ToList(); } } public IEnumerable<Person> GetPersons() { // ToList call sends query to database and DataContext releases connection return new DataContext().GetTable<Person>().ToList(); } public IQuerable<Person> GetPersons() { // query is not sent to database here // it will be executed later when user will enumerate results of method // but DataContext will handle it properly return new DataContext().GetTable<Person>(); } public async Task<IEnumerable<Person>> GetPersons() { using (var db = new DataConnection()) { // await will suspend execution inside of using waiting for query results from ToListAsync() // after that execution will continue and dispose `DataConnection` instance return await db.GetTable<Person>().ToListAsync(); } } Done wrong public IEnumerable<Person> GetPersons() { using (var db = new DataConnection()) { // query will be executed only when user will enumerate method results return db.GetTable<Person>(); } } // DataConnection already disposed here // starting from linq2db 1.8.0 it will fail with ObjectDisposedException // versions prior to 1.8.0 will execute query (if there are free connectons left) and will create leaked connection var persons = GetPersons().ToList(); public async Task<IEnumerable<Person>> GetPersons() { using (var db = new DataConnection()) { // no suspension point here, awaitable task will be returned immediately from method // creating race conditions return db.GetTable<Person>().ToListAsync(); } } // query execution will be called on disposed DataConnection var persons = await GetPersons();"
  },
  "articles/general/databases.html": {
    "href": "articles/general/databases.html",
    "title": "Supported databases | Linq To DB (aka linq2db)",
    "keywords": "Supported databases DB2 (LUW, z/OS) Firebird Informix Microsoft Access Microsoft Sql Azure Microsoft Sql Server Microsoft SqlCe MySql Oracle PostgreSQL SQLite SAP HANA Sybase ASE DB2 iSeries"
  },
  "articles/links.html": {
    "href": "articles/links.html",
    "title": "Links | Linq To DB (aka linq2db)",
    "keywords": "Links Blog LINQ to DB NuGets LINQ to DB pre release NuGets LINQ to DB on GitHub Source Code Code Examples"
  },
  "index.html": {
    "href": "index.html",
    "title": "LINQ to DB | Linq To DB (aka linq2db)",
    "keywords": "LINQ to DB LINQ to DB is the fastest LINQ database access library offering a simple, light, fast, and type-safe layer between your POCO objects and your database. Architecturally it is one step above micro-ORMs like Dapper, Massive, or PetaPoco, in that you work with LINQ expressions, not with magic strings, while maintaining a thin abstraction layer between your code and the database. Your queries are checked by the C# compiler and allow for easy refactoring. However, it's not as heavy as LINQ to SQL or Entity Framework. There is no change-tracking, so you have to manage that yourself, but on the positive side you get more control and faster access to your data. In other words LINQ to DB is type-safe SQL . Visit our blog and see Github.io documentation for more details. Code examples and demos can be found here or in tests . T4 model generation help is here . Releases and Roadmap . How to help the project No, this is not the donate link. We do need something really more valuable - your time . If you really want to help us please read this post . Project Build Status Appveyor Travis master latest Feeds NuGet MyGet V2 https://www.myget.org/F/linq2db/api/v2 V3 https://www.myget.org/F/linq2db/api/v3/index.json Let's get started From NuGet : Install-Package linq2db - .NET & .NET Core Configuring connection strings .NET In your web.config or app.config make sure you have a connection string (check this file for supported providers): <connectionStrings> <add name=\"Northwind\" connectionString = \"Server=.\\;Database=Northwind;Trusted_Connection=True;Enlist=False;\" providerName = \"SqlServer\" /> </connectionStrings> .NET Core .Net Core does not support System.Configuration so to configure connection strings you should implement ILinqToDBSettings , for example: public class ConnectionStringSettings : IConnectionStringSettings { public string ConnectionString { get; set; } public string Name { get; set; } public string ProviderName { get; set; } public bool IsGlobal => false; } public class MySettings : ILinqToDBSettings { public IEnumerable<IDataProviderSettings> DataProviders => Enumerable.Empty<IDataProviderSettings>(); public string DefaultConfiguration => \"SqlServer\"; public string DefaultDataProvider => \"SqlServer\"; public IEnumerable<IConnectionStringSettings> ConnectionStrings { get { yield return new ConnectionStringSettings { Name = \"SqlServer\", ProviderName = \"SqlServer\", ConnectionString = @\"Server=.\\;Database=Northwind;Trusted_Connection=True;Enlist=False;\" }; } } } And later just set on program startup before the first query is done (Startup.cs for example): DataConnection.DefaultSettings = new MySettings(); You can also use same for regular .NET. Now let's create a POCO class Important: you also can generate those classes from your database using T4 templates . Demonstration video could be found here . using System; using LinqToDB.Mapping; [Table(Name = \"Products\")] public class Product { [PrimaryKey, Identity] public int ProductID { get; set; } [Column(Name = \"ProductName\"), NotNull] public string Name { get; set; } // ... other columns ... } At this point LINQ to DB doesn't know how to connect to our database or which POCOs go with what database. All this mapping is done through a DataConnection class: public class DbNorthwind : LinqToDB.Data.DataConnection { public DbNorthwind() : base(\"Northwind\") { } public ITable<Product> Product => GetTable<Product>(); public ITable<Category> Category => GetTable<Category>(); // ... other tables ... } We call the base constructor with the \"Northwind\" parameter. This parameter (called configuration name ) has to match the name=\"Northwind\" we defined above in our connection string. We also have to register our Product class we defined above to allow us to write LINQ queries. And now let's get some data: using LinqToDB; using LinqToDB.Common; public static List<Product> All() { using (var db = new DbNorthwind()) { var query = from p in db.Product where p.ProductID > 25 orderby p.Name descending select p; return query.ToList(); } } Make sure you always wrap your DataConnection class (in our case DbNorthwind ) in a using statement. This is required for proper resource management, like releasing the database connections back into the pool. More details Selecting Columns Most times we get the entire row from the database: from p in db.Product where p.ProductID == 5 select p; However, sometimes getting all the fields is too wasteful so we want only certain fields, but still use our POCOs; something that is challenging for libraries that rely on object tracking, like LINQ to SQL. from p in db.Product orderby p.Name descending select new Product { Name = p.Name }; Composing queries Rather than concatenating strings we can 'compose' LINQ expressions. In the example below the final SQL will be different if onlyActive is true or false, or if searchFor is not null. public static List<Product> All(bool onlyActive, string searchFor) { using (var db = new DbNorthwind()) { var products = from p in db.Product select p; if (onlyActive) { products = from p in products where !p.Discontinued select p; } if (searchFor != null) { products = from p in products where p.Name.Contains(searchFor) select p; } return products.ToList(); } } Paging A lot of times we need to write code that returns only a subset of the entire dataset. We expand on the previous example to show what a product search function could look like. Keep in mind that the code below will query the database twice. Once to find out the total number of records, something that is required by many paging controls, and once to return the actual data. public static List<Product> Search(string searchFor, int currentPage, int pageSize, out int totalRecords) { using (var db = new DbNorthwind()) { var products = from p in db.Product select p; if (searchFor != null) { products = from p in products where p.Name.Contains(searchFor) select p; } totalRecords = products.Count(); return products.Skip((currentPage - 1) * pageSize).Take(pageSize).ToList(); } } Joins This assumes we added a Category class, just like we did with the Product class, defined all the fields, and registered it in our DbNorthwind data access class. We can now write an INNER JOIN query like this: from p in db.Product join c in db.Category on p.CategoryID equals c.CategoryID select new Product { Name = p.Name, Category = c }; and a LEFT JOIN query like this: from p in db.Product from c in db.Category.Where(q => q.CategoryID == p.CategoryID).DefaultIfEmpty() select new Product { Name = p.Name, Category = c }; More samples are here Creating your POCOs In the previous example we assign an entire Category object to our product, but what if we want all the fields in our Product class, but we don't want to specify every field by hand? Unfortunately, we cannot write this: from p in db.Product from c in db.Category.Where(q => q.CategoryID == p.CategoryID).DefaultIfEmpty() select new Product(c); The query above assumes the Product class has a constructor that takes in a Category object. The query above won't work, but we can work around that with the following query: from p in db.Product from c in db.Category.Where(q => q.CategoryID == p.CategoryID).DefaultIfEmpty() select Product.Build(p, c); For this to work, we need a function in the Product class that looks like this: public static Product Build(Product product, Category category) { if (product != null) { product.Category = category; } return product; } One caveat with this approach is that if you're using it with composed queries (see example above) the select Build part has to come only in the final select. Insert At some point we will need to add a new Product to the database. One way would be to call the Insert extension method found in the LinqToDB namespace; so make sure you import that. using (var db = new DbNorthwind()) { db.Insert(product); } This inserts all the columns from our Product class, but without retrieving the generated identity value. To do that we can use InsertWith*Identity methods, like this: using (var db = new DbNorthwind()) { product.ProductID = db.InsertWithInt32Identity(product); } There is also InsertOrReplace that updates a database record if it was found by primary key or adds it otherwise. If you need to insert only certain fields, or use values generated by the database, you could write: using (var db = new DbNorthwind()) { db.Product .Value(p => p.Name, product.Name) .Value(p => p.UnitPrice, 10.2m) .Value(p => p.Added, () => Sql.CurrentTimestamp) .Insert(); } Use of this method also allows us to build insert statements like this: using (var db = new DbNorthwind()) { var statement = db.Product .Value(p => p.Name, product.Name) .Value(p => p.UnitPrice, 10.2m); if (storeAdded) statement.Value(p => p.Added, () => Sql.CurrentTimestamp); statement.Insert(); } Update Updating records follows similar pattern to Insert. We have an extension method that updates all the columns in the database: using (var db = new DbNorthwind()) { db.Update(product); } And we also have a lower level update mechanism: using (var db = new DbNorthwind()) { db.Product .Where(p => p.ProductID == product.ProductID) .Set(p => p.Name, product.Name) .Set(p => p.UnitPrice, product.UnitPrice) .Update(); } Similarly, we can break an update query into multiple pieces if needed: using (var db = new DbNorthwind()) { var statement = db.Product .Where(p => p.ProductID == product.ProductID) .Set(p => p.Name, product.Name); if (updatePrice) statement = statement.Set(p => p.UnitPrice, product.UnitPrice); statement.Update(); } You're not limited to updating a single record. For example, we could discontinue all the products that are no longer in stock: using (var db = new DbNorthwind()) { db.Product .Where(p => p.UnitsInStock == 0) .Set(p => p.Discontinued, true) .Update(); } Delete Similar to how you update records, you can also delete records: using (var db = new DbNorthwind()) { db.Product .Where(p => p.Discontinued) .Delete(); } Bulk Copy Bulk copy feature supports the transfer of large amounts of data into a table from another data source. For faster data inserting DO NOT use a transaction. If you use a transaction an adhoc implementation of the bulk copy feature has been added in order to insert multiple lines at once. You get faster results then inserting lines one by one, but it's still slower than the database provider bulk copy. So, DO NOT use transactions whenever you can (Take care of unique constraints, primary keys, etc. since bulk copy ignores them at insertion). [Table(Name = \"ProductsTemp\")] public class ProductTemp { public int ProductID { get; set; } [Column(Name = \"ProductName\"), NotNull] public string Name { get; set; } // ... other columns ... } list = List<ProductTemp> using (var db = new DbNorthwind()) { db.BulkCopy(list); } Transactions Using database transactions is easy. All you have to do is call BeginTransaction() on your DataConnection, run one or more queries, and then commit the changes by calling CommitTransaction(). If something happened and you need to roll back your changes you can either call RollbackTransaction() or throw an exception. using (var db = new DbNorthwind()) { db.BeginTransaction(); // ... select / insert / update / delete ... if (somethingIsNotRight) { db.RollbackTransaction(); } else { db.CommitTransaction(); } } Also, you can use .NET built-in TransactionScope class: // don't forget that isolation level is serializable by default using (var transaction = new TransactionScope()) { using (var db = new DbNorthwind()) { ... } transaction.Complete(); } Merge Here you can read about MERGE support. Window (Analytic) Functions Here you can read about Window (Analytic) Functions support. MiniProfiler If you would like to use MiniProfiler from StackExchange you'd need to wrap ProfiledDbConnection around our regular DataConnection. public class DbDataContext : DataConnection { #if !DEBUG public DbDataContext() : base(\"Northwind\") { } #else public DbDataContext() : base(GetDataProvider(), GetConnection()) { } private static IDataProvider GetDataProvider() { // you can move this line to other place, but it should be // allways set before LINQ to DB provider instance creation LinqToDB.Common.Configuration.AvoidSpecificDataProviderAPI = true; return new SqlServerDataProvider(\"\", SqlServerVersion.v2012); } private static IDbConnection GetConnection() { var dbConnection = new SqlConnection(@\"Server=.\\SQL;Database=Northwind;Trusted_Connection=True;Enlist=False;\"); return new StackExchange.Profiling.Data.ProfiledDbConnection(dbConnection, MiniProfiler.Current); } #endif } This assumes that you only want to use MiniProfiler while in DEBUG mode and that you are using SQL Server for your database. If you're using a different database you would need to change GetDataProvider() to return the appropriate IDataProvider. For example for MySql you would use: private static IDataProvider GetDataProvider() { return new LinqToDB.DataProvider.MySql.MySqlDataProvider(); }"
  }
}